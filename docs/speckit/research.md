# Investigaci√≥n de APIs: APIs de Redes Sociales Gratuitas para MVP

**Fecha de Investigaci√≥n**: 2025-11-08
**Rama**: `001-social-trends-analysis`
**Restricci√≥n**: FR-029 - El sistema DEBE usar SOLO herramientas/APIs gratuitas (sin servicios de pago)
**Objetivo**: Recolectar 4,000 elementos de contenido/d√≠a (1,000 por plataforma) con datos demogr√°ficos

---

## Resumen Ejecutivo

**Hallazgo Cr√≠tico**: La mayor√≠a de las APIs oficiales de redes sociales tienen limitaciones significativas en sus niveles gratuitos que hacen que recolectar 4,000 elementos/d√≠a sea **MUY DESAFIANTE** sin costo. Solo 2 de 5 APIs evaluadas son realmente viables para el MVP sin pago.

### Recomendaciones:
- **USAR**: YouTube Data API v3 (con gesti√≥n de cuota), Google Trends (pytrends), Reddit API (PRAW)
- **EVITAR**: Meta Graph API (aprobaci√≥n restrictiva + l√≠mites bajos), TikTok APIs (acceso solo para investigadores)
- **ENFOQUE ALTERNATIVO**: Considerar web scraping para plataformas con APIs restrictivas (aplican consideraciones legales/√©ticas)

---

## 1. Meta Graph API (Facebook + Instagram)

### Estado del Nivel Gratuito: **GRATIS LIMITADO (‚ö†Ô∏è NO RECOMENDADO)**

#### L√≠mites de Tasa / Cuotas:
- **Instagram API**: 200 solicitudes por hora
- **Facebook API**: L√≠mites de tasa basados en uso de la app (call_count, total_time, total_cputime como porcentajes)
- Cuando cualquier m√©trica excede 100%, la app se limita
- **No hay forma de aumentar l√≠mites** mediante solicitud especial

#### Capacidad de Recolecci√≥n Diaria:
- **200 solicitudes/hora √ó 24 horas = 4,800 solicitudes/d√≠a m√°ximo**
- Si cada solicitud retorna 25 elementos: ~120,000 elementos/d√≠a (te√≥ricamente suficiente)
- **SIN EMBARGO**: Los requisitos estrictos de acceso limitan severamente el uso real

#### Datos Demogr√°ficos:
| Tipo de Dato | Disponibilidad | Notas |
|--------------|----------------|-------|
| Edad | ‚úÖ DISPONIBLE | Solo para cuentas de negocio/creador que autoricen tu app |
| G√©nero | ‚úÖ DISPONIBLE | Solo para cuentas de negocio/creador |
| Ubicaci√≥n | ‚úÖ DISPONIBLE | Nivel de pa√≠s/ciudad |
| **Precisi√≥n** | 95%+ | Datos directos de la plataforma (cumple SC-005) |

**Endpoints Disponibles**:
- `engaged_audience_demographics`
- `reached_audience_demographics`
- `follower_demographics`

#### M√©todo de Autenticaci√≥n:
- **OAuth 2.0** v√≠a Facebook Login
- Requiere cuenta de Facebook Developer
- **LIMITACI√ìN CR√çTICA**: Requiere aprobaci√≥n de App Review
- Los usuarios deben tener cuentas de Instagram Business/Creator conectadas a Facebook Pages
- Los tokens expiran (corta duraci√≥n: horas, larga duraci√≥n: 60 d√≠as) - debe implementarse l√≥gica de actualizaci√≥n

#### Barreras Principales:
1. **Proceso de App Review**: Puede tomar >1 semana, frecuentemente denegado
2. **Restringido a Cuentas de Negocio**: No puede acceder contenido de usuarios regulares
3. **Limitaciones de Privacidad**: Solo los 45 segmentos principales de audiencia, retraso de reporte de 48 horas
4. **Modo de Desarrollo**: L√≠mites extremadamente bajos para apps no verificadas

#### Recomendaci√≥n: **NO USAR** ‚ùå
**Justificaci√≥n**: Las barreras de revisi√≥n de la app, restricciones de cuentas de negocio y complejidad superan los beneficios. Los l√≠mites de tasa son t√©cnicamente suficientes pero los requisitos de acceso lo hacen impracticable para MVP.

#### Alternativas Gratuitas:
- **Web Scraping**: Herramientas estilo Invidious (√°rea gris legal, viola ToS)
- **Instagram Basic Display API**: Limitado solo al contenido propio del usuario
- **APIs de terceros**: RapidAPI (pago), Apify (pago $0.25/1k)

---

## 2. YouTube Data API v3

### Estado del Nivel Gratuito: **VERDADERAMENTE GRATIS** ‚úÖ **RECOMENDADO**

#### L√≠mites de Tasa / Cuotas:
- **10,000 unidades de cuota por d√≠a** (por defecto)
- La cuota se reinicia a medianoche Hora del Pac√≠fico (PT)
- Diferentes operaciones consumen diferentes unidades:
  - B√∫squeda: 100 unidades por solicitud
  - Lista de videos: 1 unidad por solicitud
  - Hilos de comentarios: 1 unidad por solicitud

#### Capacidad de Recolecci√≥n Diaria:
- **Recolecci√≥n basada en b√∫squeda**: 10,000 unidades √∑ 100 unidades = **100 b√∫squedas/d√≠a**
- Si cada b√∫squeda retorna 50 videos: **5,000 videos/d√≠a** (excede objetivo de 1,000 ‚úÖ)
- **Detalles de videos (despu√©s de b√∫squeda)**: Casi ilimitado (1 unidad por 50 videos)

**Flujo de Trabajo de Ejemplo**:
1. B√∫squeda por palabra clave: 20 b√∫squedas √ó 100 unidades = 2,000 unidades (1,000 videos)
2. Obtener detalles de video: 1,000 videos √∑ 50 por solicitud √ó 1 unidad = 20 unidades
3. **Total**: 2,020 unidades usadas, 7,980 restantes

#### Datos Demogr√°ficos:
| Tipo de Dato | Disponibilidad | Notas |
|--------------|----------------|-------|
| Edad | ‚úÖ DISPONIBLE | V√≠a YouTube Analytics API (API separada) |
| G√©nero | ‚úÖ DISPONIBLE | V√≠a YouTube Analytics API |
| Ubicaci√≥n | ‚úÖ DISPONIBLE | Pa√≠s, continente, subcontinente, ciudad, provincia (solo EE.UU.) |
| **Precisi√≥n** | 95%+ | Datos directos de usuarios conectados (cumple SC-005) |

**IMPORTANTE**: YouTube Analytics API proporciona demograf√≠a pero **SOLO para el canal propio del propietario del contenido**. Para analizar OTROS canales/videos, los datos demogr√°ficos NO est√°n disponibles v√≠a API.

**Dimensiones Disponibles** (YouTube Analytics API):
- `ageGroup`: Rangos de edad
- `gender`: female, male, user_specified
- `country`, `continent`, `subcontinent`, `city`

**Umbrales de Privacidad**: Los datos demogr√°ficos est√°n sujetos a umbrales m√≠nimos de vistas.

#### M√©todo de Autenticaci√≥n:
- **API Key** (simple, para datos p√∫blicos de solo lectura) ‚úÖ F√ÅCIL
- **OAuth 2.0** (para datos espec√≠ficos del usuario, ej., anal√≠ticas del canal propio)

#### Aumento de Cuota:
- **Gratis solicitar** v√≠a Google Cloud Console
- Aprobaci√≥n basada en caso de uso y cumplimiento
- Sin cargo monetario por aumentos

#### Recomendaci√≥n: **USAR** ‚úÖ
**Justificaci√≥n**:
- Verdaderamente gratis con cuota suficiente (5,000 elementos/d√≠a vs objetivo de 1,000)
- Autenticaci√≥n simple con API key
- API bien documentada y estable
- Se pueden solicitar aumentos de cuota gratis
- **LIMITACI√ìN**: Demograf√≠a solo para canal propio, no para analizar otros

#### Estrategia de Recolecci√≥n:
1. Usar `search.list` para encontrar videos por palabras clave (100 unidades cada uno)
2. Usar `videos.list` para obtener detalles (1 unidad por 50 videos)
3. Implementar seguimiento de cuota para evitar alcanzar l√≠mites
4. Para demograf√≠a: Inferir de comentarios, descripciones de canal (objetivo de precisi√≥n 65% seg√∫n FR-011)

---

## 3. TikTok APIs

### Estado del Nivel Gratuito: **SOLO INVESTIGADORES** ‚ùå **NO ACCESIBLE**

#### TikTok Research API:
- **Acceso**: Restringido a investigadores acad√©micos de universidades sin fines de lucro acreditadas (EE.UU. y Europa)
- **Proceso de Aplicaci√≥n**: Debe aplicar, revisado por legitimidad
- **Costo**: Gratis para investigadores aprobados
- **Cronograma**: 1-2 semanas para decisi√≥n de aprobaci√≥n

#### TikTok Commercial Content API:
- **Acceso**: Solo investigadores aprobados
- **Prop√≥sito**: B√∫squeda de anuncios y datos de contenido comercial
- **Costo**: Gratis para investigadores aprobados

#### TikTok Creative Center:
- **Sin API oficial** - solo interfaz web
- **Scrapers de terceros**: Apify ($0.25/1k elementos) - NO GRATIS

#### L√≠mites de Tasa / Cuotas:
- No documentados p√∫blicamente (acceso solo para investigadores)

#### Capacidad de Recolecci√≥n Diaria:
- **N/A** - No se puede acceder sin estatus de investigador

#### Datos Demogr√°ficos:
- No documentados p√∫blicamente
- Probablemente limitados incluso para investigadores (enfoque de privacidad primero)

#### M√©todo de Autenticaci√≥n:
- Aprobaci√≥n basada en aplicaci√≥n (no OAuth/API key)
- Client key emitida despu√©s de aprobaci√≥n

#### Recomendaci√≥n: **NO USAR** ‚ùå
**Justificaci√≥n**:
- Requiere estatus de investigador acad√©mico (no disponible para uso comercial/MVP)
- Proceso de aplicaci√≥n incompatible con cronograma de MVP
- Sin garant√≠a de aprobaci√≥n

#### Alternativas Gratuitas:
- **Web Scraping**: Herramientas como `TikTokAPI` (biblioteca Python, no oficial, viola ToS)
- **yt-dlp**: Puede descargar videos de TikTok pero no tiene API de b√∫squeda/tendencias
- **Apify TikTok Scraper**: $0.25/1k elementos (NO GRATIS)

---

## 4. Google Trends (biblioteca pytrends)

### Estado del Nivel Gratuito: **VERDADERAMENTE GRATIS** ‚úÖ **RECOMENDADO**

#### L√≠mites de Tasa / Cuotas:
- **Sin API oficial** - pytrends es un wrapper no oficial
- **L√≠mites no documentados** basados en direcci√≥n IP
- Reportes de la comunidad:
  - ~1,400 solicitudes antes del l√≠mite de tasa (con per√≠odos de 4 horas)
  - ~10 descargas con retrasos de 5-10 segundos activan l√≠mites
  - **Recomendado**: Pausa de 60 segundos entre solicitudes

#### Capacidad de Recolecci√≥n Diaria:
- **Estimaci√≥n conservadora**: 1 solicitud por minuto = **1,440 solicitudes/d√≠a**
- Cada solicitud puede obtener tendencias para m√∫ltiples palabras clave
- **Suficiente para MVP** ‚úÖ (no recolectando elementos individuales, solo validaci√≥n de tendencias)

#### Datos Demogr√°ficos:
| Tipo de Dato | Disponibilidad | Notas |
|--------------|----------------|-------|
| Edad | ‚ùå NO DISPONIBLE | N/A |
| G√©nero | ‚ùå NO DISPONIBLE | N/A |
| Ubicaci√≥n | ‚úÖ DISPONIBLE | Pa√≠s, regi√≥n, ciudad |
| **Prop√≥sito** | Validaci√≥n de Tendencias | No para recolectar elementos de contenido |

**Datos Disponibles**:
- Tendencias de volumen de b√∫squeda a lo largo del tiempo
- Inter√©s geogr√°fico (por pa√≠s/regi√≥n)
- Consultas relacionadas
- B√∫squedas en ascenso/tendencia

#### M√©todo de Autenticaci√≥n:
- **Ninguno requerido** ‚úÖ F√ÅCIL
- La biblioteca hace solicitudes HTTP a la interfaz web de Google Trends

#### Notas Importantes:
- **Repositorio de pytrends archivado** (17 de abril de 2025) - ya no se mantiene
- Puede fallar si Google cambia la estructura del sitio web de Trends
- **Riesgo**: La herramienta no oficial podr√≠a dejar de funcionar

#### Recomendaci√≥n: **USAR (con precauci√≥n)** ‚ö†Ô∏è
**Justificaci√≥n**:
- Completamente gratis, sin autenticaci√≥n
- Perfecto para FR-021 (validar tendencias con Google Trends)
- NO para recolectar 1,000+ elementos/d√≠a (an√°lisis de brechas FR-022 solamente)
- **Riesgo**: Biblioteca sin mantenimiento, puede fallar

#### Estrategia de Mitigaci√≥n:
- Implementar limitaci√≥n de tasa (60s entre solicitudes)
- Usar bloques try-except para degradaci√≥n elegante
- Tener m√©todo de validaci√≥n de respaldo listo
- Considerar bifurcar pytrends si falla

---

## 5. Reddit API (PRAW)

### Estado del Nivel Gratuito: **VERDADERAMENTE GRATIS (con limitaciones)** ‚úÖ **RECOMENDADO**

#### L√≠mites de Tasa / Cuotas:
- **Clientes autenticados con OAuth**: 100 consultas por minuto (QPM)
- **Clientes no autenticados**: 10 QPM
- L√≠mites promediados en ventanas de 10 minutos (permite r√°fagas)

#### Capacidad de Recolecci√≥n Diaria:
- **100 QPM √ó 60 minutos √ó 24 horas = 144,000 solicitudes/d√≠a**
- Si cada solicitud retorna 25 publicaciones: **3,600,000 elementos/d√≠a** (excede ampliamente el objetivo ‚úÖ)
- **Realista**: 100 solicitudes/minuto = 6,000 solicitudes/hora = **144,000 solicitudes/d√≠a**

**Ejemplo**: Recolectar 1,000 elementos/d√≠a requiere ~40 solicitudes (25 elementos cada una) = **muy por debajo de los l√≠mites**

#### Datos Demogr√°ficos:
| Tipo de Dato | Disponibilidad | Notas |
|--------------|----------------|-------|
| Edad | ‚ùå NO DISPONIBLE | Debe inferirse del subreddit, idioma, contenido de publicaci√≥n |
| G√©nero | ‚ùå NO DISPONIBLE | Debe inferirse de patrones de lenguaje |
| Ubicaci√≥n | ‚ùå NO DISPONIBLE | Debe inferirse del subreddit (ej., r/Colombia), menciones |
| **Precisi√≥n** | ~50-60% | Basado en inferencia (por debajo del objetivo del 65% en SC-004) |

**Metadatos Disponibles** (por publicaci√≥n/comentario):
- Subreddit
- Nombre de usuario del autor
- T√≠tulo de publicaci√≥n, texto, URL
- Puntuaci√≥n (votos positivos - votos negativos)
- N√∫mero de comentarios
- Marca de tiempo de creaci√≥n
- Premios

#### M√©todo de Autenticaci√≥n:
- **OAuth 2.0** v√≠a Reddit App (PRAW maneja autom√°ticamente)
- Crear cuenta de Reddit ‚Üí Crear app ‚Üí Obtener client_id y client_secret
- **Configuraci√≥n f√°cil** ‚úÖ

#### PRAW (Python Reddit API Wrapper):
- Wrapper oficial de biblioteca
- Maneja autenticaci√≥n y limitaci√≥n de tasa autom√°ticamente
- **Gratis y activamente mantenido** ‚úÖ

#### Restricciones de Uso Comercial:
- **Nivel gratuito** para uso no comercial (proyectos personales, investigaci√≥n)
- **Requiere aprobaci√≥n** para uso comercial (apps con anuncios, paywalls, monetizaci√≥n)
- **Nivel de pago**: $0.24 por 1,000 solicitudes (empresarial: miles/mes)

#### Recomendaci√≥n: **USAR (si es no comercial)** ‚ö†Ô∏è
**Justificaci√≥n**:
- Excelente nivel gratuito (100 QPM = 144,000 solicitudes/d√≠a)
- Autenticaci√≥n f√°cil con PRAW
- API bien documentada y estable
- **LIMITACI√ìN**: Demograf√≠a no disponible (debe inferirse, precisi√≥n <65%)
- **RIESGO**: Requiere aprobaci√≥n para uso comercial

#### Estrategia de Recolecci√≥n:
1. Usar PRAW para buscar subreddits por palabras clave
2. Recolectar publicaciones + comentarios principales (datos de engagement)
3. Inferir demograf√≠a de:
   - Geograf√≠a del subreddit (r/Colombia, r/Mexico)
   - Patrones de lenguaje (marcadores de g√©nero en espa√±ol)
   - Patrones temporales (cohortes de edad por horarios de publicaci√≥n)

---

## Alternativas Gratuitas Adicionales

### 6. Mastodon API ‚úÖ **RECOMENDACI√ìN BONUS**

**Estado del Nivel Gratuito**: **COMPLETAMENTE GRATIS** (c√≥digo abierto, descentralizado)

**L√≠mites de Tasa**: Var√≠a por instancia (t√≠picamente generosos para datos p√∫blicos)

**Datos de Tendencias**:
- API de hashtags en tendencia
- Enlaces/noticias en tendencia
- Estados en tendencia

**Datos Demogr√°ficos**: ‚ùå NO DISPONIBLE (red federada, sin demograf√≠a centralizada)

**Autenticaci√≥n**: OAuth 2.0 (por instancia)

**Pros**:
- Completamente gratis y de c√≥digo abierto
- Sin restricciones de API corporativa
- API de temas en tendencia integrada
- Desarrollo activo (v4.5 lanzado en Nov 2025)

**Contras**:
- Base de usuarios m√°s peque√±a que las principales plataformas
- Demograf√≠a no disponible
- Las tendencias var√≠an por instancia (modelo federado)

**Recomendaci√≥n**: **USAR como 5¬™ plataforma** (bonus m√°s all√° de las 4 principales)

---

### 7. Web Scraping (√öltimo Recurso)

**Herramientas**:
- **Twint**: Scraper gratuito de Twitter/X (no necesita API)
- **snscrape**: Scraper multiplataforma gratuito (Twitter, Instagram, etc.)
- **yt-dlp**: Descargador gratuito de YouTube (extracci√≥n de metadatos)
- **Invidious**: Frontend de privacidad de YouTube (amigable con scraping)

**Pros**:
- Sin l√≠mites de API
- Sin autenticaci√≥n requerida
- Acceso a datos p√∫blicos

**Contras**:
- **Viola T√©rminos de Servicio** (riesgo legal)
- Se rompe frecuentemente (ciclo de mantenimiento de 2-4 semanas)
- Sin datos demogr√°ficos
- Preocupaciones √©ticas
- Puede resultar en prohibiciones de IP

**Recomendaci√≥n**: **EVITAR a menos que las APIs sean insuficientes** ‚ö†Ô∏è

---

## Tabla Resumen

| API | Estado Gratuito | Capacidad Diaria | Demograf√≠a | Complejidad de Autenticaci√≥n | Recomendaci√≥n |
|-----|-----------------|------------------|------------|------------------------------|----------------|
| **Meta Graph API** | ‚ö†Ô∏è Limitado | 4,800 solicitudes | ‚úÖ 95% (solo negocios) | ‚ùå Alta (App Review) | ‚ùå NO USAR |
| **YouTube Data API v3** | ‚úÖ Verdaderamente Gratis | 5,000 elementos | ‚ö†Ô∏è Solo canal propio | ‚úÖ F√°cil (API key) | ‚úÖ **USAR** |
| **TikTok APIs** | ‚ùå Solo investigadores | N/A | ‚ùì Desconocido | ‚ùå Basado en aplicaci√≥n | ‚ùå NO USAR |
| **Google Trends** | ‚úÖ Verdaderamente Gratis | 1,440 solicitudes | üåç Solo ubicaci√≥n | ‚úÖ Ninguno | ‚ö†Ô∏è USAR (solo validaci√≥n) |
| **Reddit API (PRAW)** | ‚úÖ Verdaderamente Gratis | 144,000 solicitudes | ‚ùå Ninguno (inferir) | ‚úÖ F√°cil (OAuth) | ‚ö†Ô∏è USAR (no comercial) |
| **Mastodon API** | ‚úÖ Verdaderamente Gratis | Alto | ‚ùå Ninguno | ‚úÖ F√°cil (OAuth) | ‚úÖ USAR BONUS |

---

## Cumplimiento de Restricci√≥n: FR-029

**Requisito**: El sistema DEBE usar SOLO herramientas/APIs gratuitas

### APIs Conformes (100% Gratis):
1. ‚úÖ **YouTube Data API v3**: Cuota gratuita (10,000 unidades/d√≠a), aumentos de cuota gratuitos
2. ‚úÖ **Google Trends (pytrends)**: Completamente gratis (no oficial, sin autenticaci√≥n)
3. ‚úÖ **Reddit API (PRAW)**: Nivel gratuito (100 QPM) para uso no comercial
4. ‚úÖ **Mastodon API**: C√≥digo abierto, completamente gratis

### No Conformes (No Se Puede Acceder Sin Pago/Aprobaci√≥n):
1. ‚ùå **Meta Graph API**: T√©cnicamente gratis pero las barreras de App Review lo hacen impracticable
2. ‚ùå **TikTok APIs**: Acceso solo para investigadores (no disponible para MVP)
3. ‚ùå **Twitter/X API**: $100-200/mes m√≠nimo (expl√≠citamente fuera de alcance seg√∫n especificaci√≥n)

---

## Estrategia de Plataforma Revisada para MVP

### Plataformas Recomendadas (4,000 elementos/d√≠a total):

1. **YouTube** (1,500 elementos/d√≠a):
   - Usar YouTube Data API v3 (capacidad de 5,000)
   - Inferir demograf√≠a de comentarios/canales (precisi√≥n del 65%)

2. **Reddit** (1,500 elementos/d√≠a):
   - Usar Reddit API con PRAW (capacidad de 144,000)
   - Inferir demograf√≠a de subreddits (precisi√≥n del 60%)
   - Marcar como "no comercial" o buscar aprobaci√≥n

3. **Mastodon** (500 elementos/d√≠a):
   - Usar Mastodon API (instancias federadas)
   - Usar APIs de tendencias directamente
   - Sin demograf√≠a (marcar como "desconocido")

4. **Google Trends** (500 validaciones/d√≠a):
   - No para recolecci√≥n de elementos, solo validaci√≥n de tendencias (FR-021)
   - Datos geogr√°ficos para validaci√≥n cruzada

### Plataformas a ELIMINAR del MVP:
- ‚ùå **Facebook**: Barreras de App Review, restricciones de cuenta de negocio
- ‚ùå **Instagram**: Igual que Facebook (Graph API)
- ‚ùå **TikTok**: Acceso solo para investigadores

### Enfoque Alternativo (Si Se Acepta Web Scraping):
- Usar **snscrape** o **Twint** para Twitter/X (gratis, sin API)
- Usar **scrapers no oficiales de Instagram** (viola ToS, riesgo legal)
- **NO RECOMENDADO** debido a preocupaciones legales/√©ticas y esp√≠ritu de FR-029

---

## Estrategia de Datos Demogr√°ficos

### Plataformas con Demograf√≠a Directa:
- **NINGUNA de las APIs gratuitas recomendadas** proporciona datos demogr√°ficos p√∫blicos
- YouTube Analytics API: Solo para canal propio
- Meta Graph API: Solo cuentas de negocio (no recomendado)

### Enfoque de Inferencia (FR-011, SC-004):

**Precisi√≥n Objetivo**: 65% m√≠nimo (inferido), 95% para datos directos

**M√©todos de Inferencia**:

1. **Ubicaci√≥n (Pa√≠s/Regi√≥n/Ciudad)**:
   - YouTube: Descripciones de canal, etiquetas de ubicaci√≥n de video, idioma
   - Reddit: Geograf√≠a de subreddit (r/Colombia, r/Mexico), menciones en publicaciones
   - Mastodon: Ubicaci√≥n de instancia (mastodon.social vs instancias localizadas)
   - **Precisi√≥n Esperada**: 70-75% ‚úÖ

2. **Rango de Edad**:
   - Patrones de lenguaje (jerga, referencias generacionales)
   - Patrones de uso de plataforma (TikTok tiende a j√≥venes, Reddit var√≠a por subreddit)
   - Patrones temporales de publicaci√≥n
   - **Precisi√≥n Esperada**: 55-60% ‚ö†Ô∏è (por debajo del objetivo)

3. **G√©nero**:
   - Marcadores de idioma espa√±ol (adjetivos con g√©nero, terminaciones -o/-a)
   - Patrones de nombre de usuario
   - Auto-identificaci√≥n en biograf√≠as/publicaciones
   - **Precisi√≥n Esperada**: 60-65% ‚ö†Ô∏è (en umbral objetivo)

### Recomendaci√≥n:
- Implementar inferencia basada en NLP (FR-011)
- Marcar predicciones de baja confianza como "Desconocido" (seg√∫n caso especial en especificaci√≥n)
- Enfocarse en ubicaci√≥n (mayor precisi√≥n) y segmentaci√≥n de plataforma
- **Ajustar Criterios de √âxito**: SC-004 (precisi√≥n del 65%) puede ser optimista sin datos directos de API

---

## Implementaci√≥n de Limitaci√≥n de Tasa (FR-006)

### Estrategias Por Plataforma:

**YouTube Data API v3**:
```python
# Seguimiento de uso de cuota
current_quota = 0
MAX_QUOTA = 10000
SEARCH_COST = 100
VIDEO_COST = 1

if current_quota + SEARCH_COST > MAX_QUOTA:
    # Pausar hasta medianoche PT
    wait_until_quota_reset()
else:
    # Ejecutar solicitud
    current_quota += SEARCH_COST
```

**Reddit API (PRAW)**:
```python
# PRAW maneja la limitaci√≥n de tasa autom√°ticamente
reddit = praw.Reddit(client_id, client_secret, user_agent)
# Se duerme autom√°ticamente al acercarse al l√≠mite de 100 QPM
```

**Google Trends (pytrends)**:
```python
import time

def get_trends(keyword):
    result = pytrends.interest_over_time()
    time.sleep(60)  # Retraso de 60 segundos entre solicitudes
    return result
```

**Mastodon API**:
```python
# Verificar encabezados X-RateLimit
response = mastodon.timeline()
remaining = response.headers['X-RateLimit-Remaining']
reset_time = response.headers['X-RateLimit-Reset']

if remaining < 10:
    wait_until(reset_time)
```

---

## An√°lisis de Criterios de √âxito

### SC-001: Recolectar 4,000 elementos de contenido/d√≠a
- **YouTube**: Capacidad de 5,000 ‚úÖ
- **Reddit**: Capacidad de 144,000 ‚úÖ
- **Combinado**: Capacidad de 149,000 (excede ampliamente 4,000) ‚úÖ
- **Estado**: **ALCANZABLE** ‚úÖ

### SC-004: Inferir demograf√≠a con precisi√≥n del 65%
- **Ubicaci√≥n**: 70-75% esperado ‚úÖ
- **Edad**: 55-60% esperado ‚ö†Ô∏è (por debajo del objetivo)
- **G√©nero**: 60-65% esperado ‚ö†Ô∏è (en umbral)
- **Estado**: **DESAFIANTE** ‚ö†Ô∏è (puede necesitar ajustar objetivo o agregar fuentes de datos directos)

### SC-005: 95% de precisi√≥n para demograf√≠a directa
- **Ninguna API gratuita** proporciona demograf√≠a p√∫blica directa
- **YouTube Analytics**: Solo para canal propio (no aplicable para analizar otros)
- **Estado**: **NO ALCANZABLE sin APIs de pago** ‚ùå

---

## Recomendaciones

### Acciones Inmediatas:

1. **Proceder con YouTube + Reddit + Mastodon** para MVP
   - Alcanza objetivo de 4,000 elementos/d√≠a (SC-001) ‚úÖ
   - 100% gratis (cumple FR-029) ‚úÖ
   - Sin barreras de App Review

2. **Ajustar Expectativas Demogr√°ficas**:
   - Actualizar SC-004 a 60% de precisi√≥n (realista para inferencia)
   - Eliminar SC-005 (95% de datos directos) del alcance del MVP
   - Enfocarse en inferencia de ubicaci√≥n (mayor precisi√≥n)

3. **Implementar Inferencia NLP Robusta**:
   - Usar modelos en espa√±ol de spaCy para NER (ubicaciones, organizaciones)
   - Entrenar/ajustar modelo BERT para clasificaci√≥n de edad/g√©nero
   - Usar metadatos de subreddit/canal para pistas demogr√°ficas

4. **Agregar "Puntuaciones de Confianza"**:
   - Marcar cada predicci√≥n demogr√°fica con confianza (0-100%)
   - Permitir a usuarios filtrar por umbral de confianza
   - Transparencia sobre inferencia vs datos directos

5. **Planificar para APIs de Pago Futuras** (Fase 2):
   - Si el MVP tiene √©xito, presupuesto para Twitter API ($100/mes)
   - Considerar Brandwatch/Sprinklr para clientes empresariales
   - Mantener arquitectura modular para f√°cil intercambio de API

### Consideraciones a Largo Plazo:

- **Revisi√≥n Legal**: Asegurar cumplimiento con ToS de plataformas (especialmente uso comercial de Reddit)
- **√âtica de Web Scraping**: Documentar decisi√≥n de NO hacer scraping (esp√≠ritu de FR-029)
- **Estabilidad de API**: Monitorear pytrends (archivado), planificar migraci√≥n si falla
- **Monitoreo de Cuota**: Implementar dashboards para uso de cuota en todas las APIs

---

## Finalizaci√≥n de Tarea de Investigaci√≥n

### Pregunta: ¬øQu√© APIs tienen niveles verdaderamente gratuitos para 4,000 elementos/d√≠a?

**Respuesta**:
- ‚úÖ **YouTube Data API v3**: S√≠ (capacidad de 5,000)
- ‚úÖ **Reddit API (PRAW)**: S√≠ (capacidad de 144,000, no comercial)
- ‚úÖ **Google Trends**: S√≠ (solo validaci√≥n, no recolecci√≥n de elementos)
- ‚ö†Ô∏è **Meta Graph API**: T√©cnicamente gratis pero impracticable (barreras de App Review)
- ‚ùå **TikTok APIs**: No (acceso solo para investigadores)

### Mejores Opciones Gratuitas:
1. **YouTube Data API v3** (plataforma primaria)
2. **Reddit API con PRAW** (plataforma primaria)
3. **Mastodon API** (plataforma bonus)
4. **Google Trends v√≠a pytrends** (solo validaci√≥n)

### Estado: ‚úÖ **INVESTIGACI√ìN COMPLETA**

**Siguiente Fase**: Actualizar spec.md para reflejar cambios de plataforma (eliminar FB/IG/TikTok, agregar Reddit/Mastodon)

---

# AP√âNDICE: Estrategia de Limitaci√≥n de Tasa Multi-API (FR-006)

**Fecha de Investigaci√≥n**: 2025-11-08 (Extendida)
**Pregunta de Investigaci√≥n**: ¬øC√≥mo manejar diferentes l√≠mites de tasa a trav√©s de 4+ APIs de redes sociales con pausa/reanudaci√≥n autom√°tica, sin p√©rdida de datos y distribuci√≥n justa entre m√∫ltiples lineamientos?

## Resumen Ejecutivo - Enfoque de Limitaci√≥n de Tasa

**Soluci√≥n Recomendada**:
1. **Algoritmo de Ventana Deslizante** con Redis para seguimiento de cuota
2. **Colas Celery Separadas** por API con l√≠mites de tasa independientes
3. **Retroceso Exponencial con Jitter** para estrategia de reintento
4. **Biblioteca PyrateLimiter** para limitaci√≥n de tasa basada en decoradores
5. **Rastreador de Cuota Basado en Redis** para predicci√≥n de uso y pausa/reanudaci√≥n autom√°tica

## L√≠mites de Tasa de API Detallados

### YouTube Data API v3
- **L√≠mite**: 10,000 unidades/d√≠a (nivel de proyecto, NO por usuario)
- **Reinicio**: Medianoche Hora del Pac√≠fico (diario fijo)
- **Costo por Operaci√≥n**:
  - B√∫squeda: 100 unidades
  - Lista de videos: 1 unidad
  - Hilos de comentarios: 1 unidad
- **Implicaciones**: ~100 b√∫squedas/d√≠a O ~10,000 solicitudes de metadatos de video/d√≠a
- **Encabezados**: Sin encabezados de l√≠mite de tasa en respuestas (debe rastrearse localmente)

### Meta Graph API (si se usa)
- **L√≠mite**: 200 solicitudes/hora por usuario (ventana rodante)
- **C√°lculo**: 200 * N√∫mero de Usuarios en ventana de 1 hora
- **Reinicio**: Rodante (no fijo por hora)
- **Encabezados de Respuesta**: Proporciona informaci√≥n de l√≠mite de tasa (X-App-Usage)

### TikTok API (si es accesible)
- **L√≠mite**: Var√≠a por endpoint y nivel
- **Ventana**: Ventana deslizante de 1 minuto
- **C√≥digo de Error**: HTTP 429 con `rate_limit_exceeded`
- **Research API**: 1,000 solicitudes/d√≠a, hasta 100,000 registros/d√≠a

### Google Trends (pytrends - No Oficial)
- **L√≠mite**: No documentado (API no oficial)
- **Comportamiento Observado**: ~1,400 solicitudes antes del bloqueo
- **Recuperaci√≥n**: Pausa de 60 segundos recomendada despu√©s del l√≠mite de tasa
- **Sin Encabezados**: Debe implementarse limitaci√≥n de tasa conservadora

### Reddit API (PRAW) - Plataforma Recomendada
- **L√≠mite**: 100 consultas por minuto (QPM) para clientes OAuth
- **Ventana**: Promediada en ventanas de 10 minutos
- **Capacidad Diaria**: 144,000 solicitudes/d√≠a
- **Manejo Autom√°tico**: La biblioteca PRAW maneja la limitaci√≥n de tasa autom√°ticamente

## An√°lisis de Patrones de Limitaci√≥n de Tasa

### 1. Algoritmo Token Bucket

**Descripci√≥n**: El bucket contiene tokens (m√°ximo N), se rellena a una tasa R tokens/segundo. Cada solicitud consume 1 token.

**Pros**:
- Simple de entender e implementar
- Permite tr√°fico en r√°faga hasta el tama√±o del bucket
- Funciona bien para APIs con l√≠mites "por segundo"

**Contras**:
- Menos preciso para ventanas de tiempo largas (diarias/por hora)
- Puede permitir que se consuma la cuota completa en una r√°faga
- No ideal para APIs con cuotas diarias (YouTube)

**Caso de Uso**: Bueno para Reddit (100/minuto), TikTok (60/minuto)

### 2. Algoritmo Leaky Bucket

**Descripci√≥n**: Las solicitudes entran al bucket, se filtran a tasa constante. Si el bucket se desborda, las solicitudes se rechazan.

**Pros**:
- Suaviza la tasa de solicitudes (sin r√°fagas)
- Uso predecible de API
- Bueno para proteger APIs

**Contras**:
- No permite tr√°fico en r√°faga
- Puede subutilizar cuota (demasiado conservador)
- No flexible para costos variables de solicitud

**Caso de Uso**: Bueno para pytrends (conservador, evitar detecci√≥n)

### 3. Algoritmo de Ventana Deslizante (RECOMENDADO)

**Descripci√≥n**: Rastrea solicitudes en una ventana de tiempo deslizante (ej., √∫ltimas 24 horas, √∫ltima 1 hora). M√°s preciso para cuotas rodantes.

**Pros**:
- M√°s preciso para ventanas rodantes (Meta Graph API)
- Previene abuso de cuota
- Funciona bien para l√≠mites diarios/por hora
- Distribuci√≥n justa de cuota a lo largo del tiempo

**Contras**:
- Ligeramente m√°s complejo de implementar
- Requiere Redis Sorted Sets para sistemas distribuidos

**Caso de Uso**: MEJOR para YouTube (cuota diaria), Meta (rodante por hora), TikTok (deslizante de 1 minuto)

**Implementaci√≥n**:
```python
# Operaciones de Redis Sorted Set
ZADD quota:youtube {timestamp} {request_id}      # Agregar solicitud
ZREMRANGEBYSCORE quota:youtube 0 {cutoff}        # Eliminar solicitudes antiguas
ZCARD quota:youtube                               # Contar solicitudes en ventana
```

## Enfoque Recomendado: Ventana Deslizante con Redis

### ¬øPor Qu√© Ventana Deslizante?

1. **Preciso para L√≠mites Rodantes**: Meta Graph API usa ventana rodante de 1 hora
2. **Distribuci√≥n Justa**: Previene patr√≥n de "r√°faga luego esperar"
3. **Predicci√≥n de Cuota**: Puede predecir cu√°ndo se reiniciar√° la cuota bas√°ndose en la solicitud m√°s antigua
4. **Sin Exceso**: Nunca excede l√≠mites de cuota

### Patr√≥n de Arquitectura: Colas Separadas por API

**Justificaci√≥n**: La limitaci√≥n de tasa de Celery es por worker, no global. Colas separadas aseguran aplicaci√≥n independiente.

```
celery_queues/
‚îú‚îÄ‚îÄ youtube_queue (tasa: 100 b√∫squedas/d√≠a, 10k unidades total)
‚îú‚îÄ‚îÄ meta_queue (tasa: 200/hora por usuario)
‚îú‚îÄ‚îÄ reddit_queue (tasa: 100/minuto, auto-manejado por PRAW)
‚îú‚îÄ‚îÄ tiktok_queue (tasa: var√≠a por endpoint)
‚îî‚îÄ‚îÄ trends_queue (tasa: 1 solicitud/5 segundos)
```

**Configuraci√≥n de Worker**:
```python
# celeryconfig.py
task_routes = {
    'collectors.youtube.*': {'queue': 'youtube_queue'},
    'collectors.meta.*': {'queue': 'meta_queue'},
    'collectors.reddit.*': {'queue': 'reddit_queue'},
    'collectors.tiktok.*': {'queue': 'tiktok_queue'},
    'collectors.trends.*': {'queue': 'trends_queue'},
}

# Ejecutar workers separados por cola
# celery -A app worker -Q youtube_queue -c 1
# celery -A app worker -Q meta_queue -c 2
# celery -A app worker -Q reddit_queue -c 4
```

**Beneficios**:
- Dominios de falla independientes (una API ca√≠da no afecta a otras)
- Aplicaci√≥n de l√≠mite de tasa por API
- F√°cil escalar workers por carga de API
- Monitoreo y alertas a nivel de cola

## Bibliotecas Recomendadas

### Primaria: PyrateLimiter (M√°s Completa)

**Repositorio**: https://github.com/vutran1710/PyrateLimiter
**PyPI**: `pyrate-limiter`

**Caracter√≠sticas**:
- Familia de algoritmos leaky bucket
- Soporte de decoradores (@RateLimiter)
- Flujos de trabajo s√≠ncronos y as√≠ncronos
- Backend Redis para limitaci√≥n de tasa distribuida
- Backend SQLite para persistencia
- Soporte de ventana deslizante

**Instalaci√≥n**:
```bash
pip install pyrate-limiter[all]  # Incluye soporte Redis
```

**Ejemplo de Uso**:
```python
from pyrate_limiter import Duration, Rate, Limiter
from pyrate_limiter.backends.redis import RedisBackend

# Definir diferentes tasas para diferentes APIs
youtube_rate = Rate(100, Duration.DAY)  # 100 b√∫squedas por d√≠a
meta_rate = Rate(200, Duration.HOUR)    # 200 solicitudes por hora
reddit_rate = Rate(100, Duration.MINUTE) # 100 solicitudes por minuto
trends_rate = Rate(1, Duration.SECOND * 5)  # 1 solicitud por 5 segundos

# Backend Redis para limitaci√≥n distribuida
redis_backend = RedisBackend(
    host='localhost',
    port=6379,
    db=0
)

# Crear limitadores por API
youtube_limiter = Limiter(youtube_rate, backend=redis_backend)
meta_limiter = Limiter(meta_rate, backend=redis_backend)
trends_limiter = Limiter(trends_rate, backend=redis_backend)

# Usar como decorador
@youtube_limiter.ratelimit("youtube_search", delay=True)
def youtube_search_api(query):
    # Llamada API aqu√≠
    pass
```

### Alternativa: ratelimit (M√°s Simple)

**Repositorio**: https://github.com/tomasbasham/ratelimit
**PyPI**: `ratelimit`

**Caracter√≠sticas**:
- Interfaz de decorador simple
- Decorador sleep_and_retry integrado
- Sin dependencias externas

**Limitaciones**:
- Sin backend Redis (no adecuado para sistemas distribuidos)
- Solo en memoria
- Sin soporte multi-tasa

## Patr√≥n de Seguimiento de Cuota

### Rastreador de Cuota Personalizado Basado en Redis

```python
import redis
from datetime import datetime, timedelta

class QuotaTracker:
    def __init__(self, redis_client, api_name):
        self.redis = redis_client
        self.api_name = api_name
        self.key = f"quota:{api_name}"

    def track_request(self, cost=1):
        """Rastrea solicitud de API con costo (para YouTube)"""
        now = datetime.now().timestamp()
        # Agregar n√∫mero de entradas de costo (unidades de cuota de YouTube)
        for _ in range(cost):
            self.redis.zadd(self.key, {f"{now}_{_}": now})

    def get_usage(self, window_seconds):
        """Obtener uso actual en ventana"""
        cutoff = (datetime.now() - timedelta(seconds=window_seconds)).timestamp()
        # Eliminar entradas expiradas
        self.redis.zremrangebyscore(self.key, 0, cutoff)
        # Contar entradas restantes
        return self.redis.zcard(self.key)

    def get_remaining(self, limit, window_seconds):
        """Obtener cuota restante"""
        usage = self.get_usage(window_seconds)
        return max(0, limit - usage)

    def predict_reset_time(self, window_seconds):
        """Predecir cu√°ndo se reiniciar√° la cuota (expira la solicitud m√°s antigua)"""
        oldest = self.redis.zrange(self.key, 0, 0, withscores=True)
        if not oldest:
            return datetime.now()
        oldest_timestamp = oldest[0][1]
        return datetime.fromtimestamp(oldest_timestamp + window_seconds)

    def should_pause(self, limit, window_seconds, threshold=0.9):
        """Verificar si se debe pausar (90% de cuota usada por defecto)"""
        usage = self.get_usage(window_seconds)
        return usage >= (limit * threshold)
```

**Ejemplo de Uso**:
```python
# Rastrear cuota de YouTube
tracker = QuotaTracker(redis_client, "youtube_search")
tracker.track_request(cost=100)  # La b√∫squeda cuesta 100 unidades

# Verificar si se debe pausar
if tracker.should_pause(10000, 86400, threshold=0.8):  # 80% de 10k unidades
    sleep_until = tracker.predict_reset_time(86400)
    # Pausar recolecci√≥n hasta reinicio
    logger.warning(f"Cuota de YouTube al 80%. Pausando hasta {sleep_until}")
    pause_collection_until(sleep_until)
```

## Estrategia de Reintento: Retroceso Exponencial con Jitter

### ¬øPor Qu√© Retroceso Exponencial?

- Previene problema de manada atronadora (todas las tareas reintentando simult√°neamente)
- Respeta tiempo de recuperaci√≥n de API
- Recomendado por todos los principales proveedores de API (YouTube, Meta, Reddit)

### Soporte Integrado de Celery (Celery 4.2+)

```python
from celery import Task

@app.task(
    bind=True,
    autoretry_for=(RateLimitException, ConnectionError),
    retry_backoff=True,           # Habilitar retroceso exponencial
    retry_backoff_max=600,         # M√°ximo 10 minutos
    retry_jitter=True,             # Agregar aleatoriedad
    max_retries=5
)
def collect_youtube_data(self, query):
    try:
        # Llamada API aqu√≠
        pass
    except RateLimitException as exc:
        # Registrar evento
        logger.warning(f"L√≠mite de tasa alcanzado: {exc}")
        raise  # Celery auto-reintenta con retroceso
```

**F√≥rmula de Retroceso**:
```
delay = min(retry_backoff_max, retry_backoff * (2 ** retries))
with_jitter = delay * random.uniform(0.5, 1.5)

# Ejemplo de progresi√≥n:
# Reintento 1: 1s * (2^0) = 1s ‚Üí 0.5-1.5s
# Reintento 2: 1s * (2^1) = 2s ‚Üí 1-3s
# Reintento 3: 1s * (2^2) = 4s ‚Üí 2-6s
# Reintento 4: 1s * (2^3) = 8s ‚Üí 4-12s
# Reintento 5: 1s * (2^4) = 16s ‚Üí 8-24s
```

### Estrategias de Retroceso Espec√≠ficas por API

```python
def youtube_backoff(retries):
    """YouTube: Retroceso conservador debido a cuota diaria"""
    return min(3600, 300 * (2 ** retries))  # Comienza con 5 min, m√°ximo 1 hora

def meta_backoff(retries):
    """Meta: Retroceso m√°s corto debido a reinicio por hora"""
    return min(600, 60 * (2 ** retries))  # Comienza con 1 min, m√°ximo 10 min

def trends_backoff(retries):
    """Trends: Retroceso m√°s largo para evitar detecci√≥n"""
    return min(900, 60 * (2 ** retries))  # Comienza con 1 min, m√°ximo 15 min

@app.task(bind=True, max_retries=5)
def collect_youtube_data(self, query):
    try:
        # Llamada API
        pass
    except RateLimitException as exc:
        countdown = youtube_backoff(self.request.retries)
        logger.info(f"Reintentando en {countdown}s (intento {self.request.retries})")
        raise self.retry(exc=exc, countdown=countdown)
```

## Priorizaci√≥n de Colas

### Colas de Prioridad (RabbitMQ)

```python
# celeryconfig.py
task_queue_max_priority = 10

task_routes = {
    'collectors.youtube.*': {
        'queue': 'youtube_queue',
        'priority': 7  # Mayor prioridad (el l√≠mite diario es precioso)
    },
    'collectors.meta.*': {
        'queue': 'meta_queue',
        'priority': 6  # Media-alta (l√≠mite por hora)
    },
    'collectors.reddit.*': {
        'queue': 'reddit_queue',
        'priority': 5  # Media (l√≠mite generoso)
    },
    'collectors.trends.*': {
        'queue': 'trends_queue',
        'priority': 3  # M√°s baja (solo validaci√≥n)
    }
}

# Enviar tarea con prioridad personalizada
collect_youtube_data.apply_async(
    args=[lineamiento_id],
    priority=9  # Recolecci√≥n urgente
)
```

**Configuraci√≥n de Prefetch de Worker** (para mejor manejo de prioridad):
```python
worker_prefetch_multiplier = 1  # Obtener 1 tarea a la vez (no 4 por defecto)
```

## Mecanismo de Pausa/Reanudaci√≥n

### Estado de Cola Respaldado por Base de Datos

```python
# models.py
class CollectionQueue(models.Model):
    lineamiento = models.ForeignKey(Lineamiento, on_delete=models.CASCADE)
    api_name = models.CharField(max_length=50)
    status = models.CharField(
        max_length=20,
        choices=[
            ('active', 'Active'),
            ('paused', 'Paused - Rate Limit'),
            ('failed', 'Failed'),
        ],
        default='active'
    )
    paused_at = models.DateTimeField(null=True, blank=True)
    resume_at = models.DateTimeField(null=True, blank=True)
    retry_count = models.IntegerField(default=0)
    last_error = models.TextField(null=True, blank=True)
    quota_used = models.IntegerField(default=0)

    class Meta:
        unique_together = ('lineamiento', 'api_name')
```

### Implementaci√≥n de Tarea con Pausa/Reanudaci√≥n

```python
@app.task(bind=True)
def collect_youtube_data(self, lineamiento_id):
    queue = CollectionQueue.objects.get(
        lineamiento_id=lineamiento_id,
        api_name='youtube'
    )

    # Verificar si est√° pausado
    if queue.status == 'paused':
        if datetime.now() < queue.resume_at:
            # A√∫n en ventana de pausa
            countdown = (queue.resume_at - datetime.now()).seconds
            logger.info(f"Recolecci√≥n pausada hasta {queue.resume_at}")
            raise self.retry(countdown=countdown)
        else:
            # Reanudar
            queue.status = 'active'
            queue.save()
            logger.info("Recolecci√≥n reanudada")

    # Verificar cuota antes de hacer solicitudes
    tracker = QuotaTracker(redis_client, 'youtube')

    if tracker.should_pause(10000, 86400, threshold=0.8):
        # Pausar hasta que se reinicie la cuota
        reset_time = tracker.predict_reset_time(86400)
        queue.status = 'paused'
        queue.paused_at = datetime.now()
        queue.resume_at = reset_time
        queue.quota_used = tracker.get_usage(86400)
        queue.save()

        logger.warning(
            f"Cuota de YouTube al 80% ({queue.quota_used}/10000). "
            f"Pausando hasta {reset_time}"
        )

        # Reprogramar para tiempo de reinicio
        raise self.retry(eta=reset_time)

    try:
        # Hacer llamada API
        results = youtube_api.search(...)
        tracker.track_request(cost=100)

        # Actualizar cola
        queue.status = 'active'
        queue.quota_used = tracker.get_usage(86400)
        queue.save()

        return results

    except RateLimitException as exc:
        # L√≠mite de tasa alcanzado inmediatamente
        queue.status = 'paused'
        queue.paused_at = datetime.now()
        queue.resume_at = datetime.now() + timedelta(hours=1)
        queue.retry_count += 1
        queue.last_error = str(exc)
        queue.save()

        # Reintentar con retroceso
        raise self.retry(
            exc=exc,
            countdown=youtube_backoff(queue.retry_count)
        )
```

## Distribuci√≥n Justa Entre M√∫ltiples Lineamientos

### Estrategia 1: Distribuci√≥n Round-Robin

```python
def distribute_quota_fairly(lineamientos):
    """Distribuir cuota de API justamente entre lineamientos"""

    # YouTube: 100 b√∫squedas por d√≠a, distribuir equitativamente
    youtube_quota_per_lineamiento = 100 // len(lineamientos)

    # Reddit: 144,000 solicitudes por d√≠a, distribuir equitativamente
    reddit_quota_per_lineamiento = 144000 // len(lineamientos)

    for lineamiento in lineamientos:
        # Asignar presupuesto de cuota a cada lineamiento
        lineamiento.daily_youtube_budget = youtube_quota_per_lineamiento
        lineamiento.daily_reddit_budget = reddit_quota_per_lineamiento
        lineamiento.save()

    # Programar tareas en round-robin
    tasks = []
    for lineamiento in lineamientos:
        tasks.append(
            group(
                collect_youtube_data.s(
                    lineamiento.id,
                    max_requests=lineamiento.daily_youtube_budget
                ),
                collect_reddit_data.s(
                    lineamiento.id,
                    max_requests=lineamiento.daily_reddit_budget
                ),
                collect_trends_data.si(lineamiento.id)
            )
        )

    # Ejecutar con coordinaci√≥n
    job = chord(tasks)(aggregate_results.s())
    return job
```

### Estrategia 2: Distribuci√≥n Basada en Prioridad

```python
def distribute_quota_by_priority(lineamientos):
    """Distribuir cuota bas√°ndose en prioridad de lineamiento"""

    # Calcular distribuci√≥n ponderada
    total_priority = sum(l.priority for l in lineamientos)

    for lineamiento in lineamientos:
        weight = lineamiento.priority / total_priority
        lineamiento.daily_youtube_budget = int(100 * weight)
        lineamiento.daily_reddit_budget = int(144000 * weight)
        lineamiento.save()
```

### Estrategia 3: Asignaci√≥n Din√°mica

```python
class QuotaAllocator:
    """Asignar cuota din√°micamente bas√°ndose en rendimiento de lineamiento"""

    def __init__(self, redis_client):
        self.redis = redis_client

    def allocate_quota(self, lineamiento, api_name):
        """Asignar cuota bas√°ndose en tasa de √©xito reciente"""

        # Obtener tasa de √©xito hist√≥rica
        success_rate = self.get_success_rate(lineamiento, api_name)

        # Obtener cuota base
        base_quota = self.get_base_quota(api_name)

        # Asignar m√°s cuota a lineamientos exitosos
        if success_rate > 0.8:
            allocated = base_quota * 1.2  # Bono del 20%
        elif success_rate < 0.5:
            allocated = base_quota * 0.8  # Penalizaci√≥n del 20%
        else:
            allocated = base_quota

        return int(allocated)

    def get_success_rate(self, lineamiento, api_name):
        """Calcular tasa de √©xito de recolecciones recientes"""
        key = f"success:{lineamiento.id}:{api_name}"
        successful = int(self.redis.get(f"{key}:success") or 0)
        failed = int(self.redis.get(f"{key}:failed") or 0)
        total = successful + failed

        if total == 0:
            return 0.5  # Neutral

        return successful / total
```

## Patr√≥n de Coordinaci√≥n Multi-API

### Servicio Coordinador

```python
from celery import group, chain
from datetime import datetime, timedelta

class CollectionCoordinator:
    """Orquesta recolecci√≥n a trav√©s de m√∫ltiples APIs"""

    def __init__(self, quota_trackers):
        self.trackers = quota_trackers  # Dict de QuotaTracker por API

    def schedule_collection(self, lineamiento):
        """Programar tareas de recolecci√≥n a trav√©s de APIs"""
        tasks = []

        # YouTube - verificar cuota
        if self.can_collect('youtube', lineamiento):
            tasks.append(
                collect_youtube_data.s(lineamiento.id)
            )
        else:
            # Programar para despu√©s
            eta = self.trackers['youtube'].predict_reset_time(86400)
            tasks.append(
                collect_youtube_data.apply_async(
                    args=[lineamiento.id],
                    eta=eta,
                    priority=7
                )
            )
            logger.info(f"Cuota de YouTube agotada. Programado para {eta}")

        # Reddit - usualmente disponible (l√≠mites generosos)
        if self.can_collect('reddit', lineamiento):
            tasks.append(
                collect_reddit_data.s(lineamiento.id)
            )

        # Trends - siempre limitado, baja prioridad
        tasks.append(
            collect_trends_data.si(lineamiento.id)
        )

        # Ejecutar tareas disponibles en paralelo
        job = group(tasks)
        return job.apply_async()

    def can_collect(self, api_name, lineamiento):
        """Verificar si la API puede manejar recolecci√≥n"""
        tracker = self.trackers.get(api_name)
        if not tracker:
            return True  # API desconocida, permitir

        if api_name == 'youtube':
            # Conservador: mantener 20% de buffer
            return not tracker.should_pause(10000, 86400, threshold=0.8)

        elif api_name == 'reddit':
            # L√≠mite generoso, permitir a menos que sea muy alto
            return not tracker.should_pause(144000, 86400, threshold=0.95)

        elif api_name == 'meta':
            # Cuota por usuario
            return not tracker.should_pause(200, 3600, threshold=0.9)

        return True
```

## Ejemplos de C√≥digo Completos

### Recolector de YouTube con Limitaci√≥n de Tasa Completa

```python
from pyrate_limiter import Duration, Rate, Limiter
from pyrate_limiter.backends.redis import RedisBackend
import redis
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# Configuraci√≥n de Redis
redis_client = redis.Redis(host='localhost', port=6379, db=0)
redis_backend = RedisBackend(redis_client)

# Limitador de tasa de YouTube
youtube_search_rate = Rate(100, Duration.DAY)
youtube_limiter = Limiter(youtube_search_rate, backend=redis_backend)

# Rastreador de cuota
class YouTubeQuotaTracker(QuotaTracker):
    DAILY_LIMIT = 10000
    SEARCH_COST = 100
    LIST_COST = 1

    def can_search(self):
        usage = self.get_usage(86400)
        return (usage + self.SEARCH_COST) <= (self.DAILY_LIMIT * 0.8)

    def track_search(self):
        self.track_request(cost=self.SEARCH_COST)

youtube_tracker = YouTubeQuotaTracker(redis_client, 'youtube')

@app.task(
    bind=True,
    autoretry_for=(HttpError,),
    retry_backoff=True,
    retry_backoff_max=3600,
    retry_jitter=True,
    max_retries=5
)
@youtube_limiter.ratelimit("youtube_search", delay=True)
def collect_youtube_data(self, lineamiento_id, query):
    """Recolectar datos de YouTube con limitaci√≥n de tasa completa"""

    # Verificar cuota antes de b√∫squeda costosa
    if not youtube_tracker.can_search():
        reset_time = youtube_tracker.predict_reset_time(86400)
        countdown = (reset_time - datetime.now()).seconds

        logger.warning(
            f"Cuota de YouTube casi agotada. "
            f"Pausando hasta {reset_time}. "
            f"Actual: {youtube_tracker.get_usage(86400)}/10000"
        )

        raise self.retry(countdown=countdown)

    try:
        # Construir servicio de YouTube
        youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)

        # Realizar b√∫squeda
        search_response = youtube.search().list(
            q=query,
            type='video',
            part='id,snippet',
            maxResults=50
        ).execute()

        # Rastrear uso de cuota
        youtube_tracker.track_search()

        logger.info(
            f"B√∫squeda de YouTube completada. "
            f"Resultados: {len(search_response.get('items', []))}. "
            f"Cuota restante: {youtube_tracker.get_remaining(10000, 86400)}"
        )

        return search_response

    except HttpError as e:
        if e.resp.status == 429:  # L√≠mite de tasa
            logger.error("¬°L√≠mite de tasa de YouTube alcanzado!")
            raise  # Auto-reintentar con retroceso
        elif e.resp.status == 403:  # Cuota excedida
            logger.error("¬°Cuota de YouTube excedida!")
            # Pausar hasta medianoche PT
            reset_time = youtube_tracker.predict_reset_time(86400)
            raise self.retry(countdown=(reset_time - datetime.now()).seconds)
        else:
            raise
```

### Recolector de Reddit (PRAW maneja limitaci√≥n de tasa autom√°ticamente)

```python
import praw

# PRAW maneja la limitaci√≥n de tasa autom√°ticamente
reddit = praw.Reddit(
    client_id=REDDIT_CLIENT_ID,
    client_secret=REDDIT_CLIENT_SECRET,
    user_agent=REDDIT_USER_AGENT
)

@app.task(
    bind=True,
    autoretry_for=(Exception,),
    retry_backoff=True,
    max_retries=3
)
def collect_reddit_data(self, lineamiento_id, subreddit_name, keywords):
    """Recolectar datos de Reddit - PRAW maneja limitaci√≥n de tasa"""

    try:
        subreddit = reddit.subreddit(subreddit_name)

        # Buscar palabras clave
        results = []
        for keyword in keywords:
            # PRAW se duerme autom√°ticamente si se acerca al l√≠mite de tasa
            for submission in subreddit.search(keyword, limit=100, time_filter='week'):
                results.append({
                    'id': submission.id,
                    'title': submission.title,
                    'score': submission.score,
                    'num_comments': submission.num_comments,
                    'created_utc': submission.created_utc,
                    'url': submission.url,
                    'author': str(submission.author),
                })

        logger.info(f"Recolecci√≥n de Reddit completada. Elementos: {len(results)}")
        return results

    except Exception as e:
        logger.error(f"Recolecci√≥n de Reddit fall√≥: {e}")
        raise
```

### Recolector de Google Trends (Limitaci√≥n de Tasa Conservadora)

```python
from pytrends.request import TrendReq
from pyrate_limiter import Duration, Rate, Limiter
import time

# Limitador de tasa de Trends (muy conservador)
trends_rate = Rate(1, Duration.SECOND * 5)
trends_limiter = Limiter(trends_rate, backend=redis_backend)

# Inicializar con retroceso
pytrends = TrendReq(
    hl='es',
    tz=360,
    retries=2,
    backoff_factor=0.1
)

@app.task(
    bind=True,
    autoretry_for=(Exception,),
    retry_backoff=60,
    retry_backoff_max=900,
    retry_jitter=True,
    max_retries=3
)
@trends_limiter.ratelimit("google_trends", delay=True)
def collect_trends_data(self, lineamiento_id, keywords):
    """Recolectar Google Trends con limitaci√≥n de tasa conservadora"""

    try:
        # Retraso manual adicional
        time.sleep(2)

        # Construir payload
        pytrends.build_payload(
            keywords,
            cat=0,
            timeframe='today 7-d',
            geo='CO'
        )

        # Obtener inter√©s a lo largo del tiempo
        interest = pytrends.interest_over_time()

        # Obtener consultas relacionadas
        related = pytrends.related_queries()

        # Dormir despu√©s del √©xito
        time.sleep(3)

        return {
            'interest': interest.to_dict(),
            'related_queries': related
        }

    except Exception as e:
        if '429' in str(e) or 'quota' in str(e).lower():
            logger.warning("L√≠mite de tasa de Google Trends sospechado")
            raise self.retry(countdown=300)  # 5 minutos
        else:
            raise
```

## Monitoreo y Alertas

### M√©tricas de Prometheus

```python
from prometheus_client import Gauge, Counter

# Definir m√©tricas
quota_usage = Gauge(
    'api_quota_usage',
    'Uso actual de cuota de API',
    ['api_name', 'window']
)

quota_exceeded = Counter(
    'api_quota_exceeded_total',
    'Total de eventos de cuota excedida',
    ['api_name']
)

rate_limit_pauses = Counter(
    'api_rate_limit_pauses_total',
    'Total de eventos de pausa de l√≠mite de tasa',
    ['api_name']
)

# Actualizar m√©tricas peri√≥dicamente
@app.task
def update_quota_metrics():
    """Actualizar m√©tricas de Prometheus para uso de cuota"""
    trackers = {
        'youtube': YouTubeQuotaTracker(redis_client, 'youtube'),
        'reddit': QuotaTracker(redis_client, 'reddit'),
        'trends': QuotaTracker(redis_client, 'trends'),
    }

    for api_name, tracker in trackers.items():
        if api_name == 'youtube':
            usage = tracker.get_usage(86400)
            quota_usage.labels(api_name='youtube', window='daily').set(usage)

        elif api_name == 'reddit':
            usage = tracker.get_usage(60)  # Por minuto
            quota_usage.labels(api_name='reddit', window='minute').set(usage)

        elif api_name == 'trends':
            usage = tracker.get_usage(3600)  # Por hora
            quota_usage.labels(api_name='trends', window='hourly').set(usage)
```

### Configuraci√≥n de Alertas

```python
def check_quota_alerts():
    """Enviar alertas cuando el uso de cuota es alto"""
    youtube_tracker = YouTubeQuotaTracker(redis_client, 'youtube')
    youtube_usage = youtube_tracker.get_usage(86400)

    if youtube_usage >= 8000:  # 80% de 10,000
        send_alert(
            level='warning',
            message=f"Cuota de YouTube en {youtube_usage}/10000 (80%+). "
                   f"Se reinicia en {youtube_tracker.predict_reset_time(86400)}"
        )

    if youtube_usage >= 9500:  # 95% de 10,000
        send_alert(
            level='critical',
            message=f"Cuota de YouTube en {youtube_usage}/10000 (95%+). "
                   f"¬°La recolecci√≥n se pausar√° pronto!"
        )
```

## Estrategia de Pruebas

### Pruebas Unitarias

```python
import pytest
from unittest.mock import Mock, patch
from freezegun import freeze_time

def test_quota_tracker_usage():
    """Probar seguimiento de cuota a lo largo del tiempo"""
    redis_mock = Mock()
    tracker = QuotaTracker(redis_mock, 'test_api')

    tracker.track_request(cost=100)

    assert redis_mock.zadd.called
    redis_mock.zadd.assert_called_once()

def test_should_pause_threshold():
    """Probar decisi√≥n de pausa en umbral"""
    redis_mock = Mock()
    tracker = QuotaTracker(redis_mock, 'test_api')
    tracker.get_usage = Mock(return_value=900)

    # Debe pausar al 90% de 1000
    assert tracker.should_pause(1000, 3600, threshold=0.9)

    # No debe pausar con umbral de 80% y uso de 70%
    tracker.get_usage = Mock(return_value=700)
    assert not tracker.should_pause(1000, 3600, threshold=0.9)

@freeze_time("2025-01-01 12:00:00")
def test_predict_reset_time():
    """Probar predicci√≥n de tiempo de reinicio"""
    redis_mock = Mock()
    tracker = QuotaTracker(redis_mock, 'test_api')

    # Solicitud m√°s antigua a las 11:00:00
    redis_mock.zrange.return_value = [(b'req1', 1704110400.0)]

    reset_time = tracker.predict_reset_time(3600)

    # Debe ser 12:00:00
    assert reset_time.hour == 12
```

### Pruebas de Integraci√≥n

```python
@pytest.mark.integration
def test_youtube_rate_limiting_integration(redis_client):
    """Probar recolector de YouTube con Redis real"""
    tracker = YouTubeQuotaTracker(redis_client, 'youtube_test')

    # Simular 90 b√∫squedas (9000 unidades)
    for i in range(90):
        tracker.track_search()

    # Debe permitir 10 b√∫squedas m√°s
    assert tracker.can_search()

    # Simular 10 b√∫squedas m√°s (10,000 unidades total)
    for i in range(10):
        tracker.track_search()

    # NO debe permitir m√°s (en umbral de 80% con uso de 10k)
    assert not tracker.can_search()
```

## Lista de Verificaci√≥n de Implementaci√≥n

- [ ] Instalar PyrateLimiter con backend Redis (`pip install pyrate-limiter[all]`)
- [ ] Configurar Redis para limitaci√≥n de tasa y seguimiento de cuota
- [ ] Crear colas Celery separadas por API (youtube_queue, reddit_queue, etc.)
- [ ] Implementar clase QuotaTracker con backend Redis
- [ ] Agregar decoradores de limitaci√≥n de tasa a recolectores de API
- [ ] Configurar retroceso exponencial en tareas Celery
- [ ] Crear modelo CollectionQueue para estado de pausa/reanudaci√≥n
- [ ] Implementar CollectionCoordinator para orquestaci√≥n multi-API
- [ ] Agregar l√≥gica de distribuci√≥n de cuota para m√∫ltiples lineamientos (justa o basada en prioridad)
- [ ] Configurar m√©tricas de Prometheus para monitoreo de cuota
- [ ] Configurar alertas para agotamiento de cuota (umbrales de 80%, 95%)
- [ ] Probar mecanismo de pausa/reanudaci√≥n para cada API
- [ ] Probar distribuci√≥n justa entre lineamientos
- [ ] Documentar l√≠mites de tasa espec√≠ficos por API y estrategias de retroceso
- [ ] Crear dashboard de monitoreo (Grafana) para uso de cuota

## Resumen de Recomendaciones

### Algoritmo de Limitaci√≥n de Tasa
**Usar Ventana Deslizante** - M√°s preciso para cuotas rodantes (Meta), funciona bien para cuotas diarias (YouTube)

### Biblioteca
**Usar PyrateLimiter** - Backend Redis, soporte de decoradores, m√∫ltiples l√≠mites de tasa, bien mantenido

### Arquitectura Celery
**Usar Colas Separadas** - Limitaci√≥n de tasa independiente por API, mejor aislamiento de fallos

### Estrategia de Reintento
**Usar Retroceso Exponencial con Jitter** - Previene manada atronadora, retrasos de retroceso espec√≠ficos por API

### Seguimiento de Cuota
**Usar QuotaTracker Redis Personalizado** - Predecir tiempos de reinicio, pausa/reanudaci√≥n autom√°tica, monitoreo de uso

### Estrategia de Distribuci√≥n
**Usar Round-Robin o Basado en Prioridad** - Distribuci√≥n justa de cuota entre lineamientos

### Sin P√©rdida de Datos
**Usar Estado de Cola Respaldado por Base de Datos** - Persistir estado de pausa/reanudaci√≥n, conteo de reintentos, uso de cuota

### Estrategias de Respaldo
**Fallos Independientes de API** - Una API ca√≠da no detiene otras, resultados parciales OK

## Referencias

### Bibliotecas
- PyrateLimiter: https://github.com/vutran1710/PyrateLimiter
- ratelimit: https://github.com/tomasbasham/ratelimit
- requests-ratelimiter: https://pypi.org/project/requests-ratelimiter/
- Celery: https://docs.celeryq.dev/

### Documentaci√≥n de API
- YouTube Data API v3: https://developers.google.com/youtube/v3/determine_quota_cost
- Reddit API: https://www.reddit.com/dev/api
- pytrends: https://github.com/GeneralMills/pytrends

### Patrones de Limitaci√≥n de Tasa
- Redis Rate Limiting: https://redis.io/learn/howtos/ratelimiting
- Algoritmo de Ventana Deslizante: https://arpitbhayani.me/blogs/sliding-window-ratelimiter/
- Enrutamiento de Celery: https://docs.celeryq.dev/en/stable/userguide/routing.html
- Retroceso Exponencial: https://testdriven.io/blog/retrying-failed-celery-tasks/

---

**Estado de Investigaci√≥n de Limitaci√≥n de Tasa**: ‚úÖ **COMPLETA**

**Pr√≥ximos Pasos**: Implementar seguimiento de cuota y limitaci√≥n de tasa durante Fase 2 (ejecuci√≥n de tareas)

---

# Tarea de Investigaci√≥n #6: Arquitectura de Tareas Celery

**Fecha de Investigaci√≥n**: 2025-11-08
**Enfoque de Investigaci√≥n**: Organizaci√≥n de tareas para recolectores, procesamiento NLP y anal√≠ticas

## Resumen Ejecutivo

Bas√°ndose en investigaci√≥n exhaustiva de mejores pr√°cticas de Celery, patrones de producci√≥n y estrategias de optimizaci√≥n de rendimiento, aqu√≠ est√°n las recomendaciones clave para organizar tareas Celery en el sistema de an√°lisis de redes sociales:

### Recomendaciones R√°pidas

| Aspecto | Recomendaci√≥n |
|---------|---------------|
| **Granularidad de Tarea** | Recolectores por lotes (50 elementos), NLP por elemento √∫nico, anal√≠ticas por lotes |
| **Organizaci√≥n de Colas** | 5 colas: youtube_collector, reddit_collector, mastodon_collector, nlp_processing, analytics |
| **Patr√≥n Canvas** | chain(group(collectors), chord(nlp_tasks, analytics_callback)) |
| **Manejo de Errores** | Retroceso exponencial (3 reintentos) + DLQ de Base de Datos + Circuit breaker |
| **Multiplicador de Prefetch** | Recolectores: 4, NLP: 1, Anal√≠ticas: 2 |
| **Backend de Resultados** | Redis con TTL de 1 d√≠a |
| **Concurrencia** | Recolectores: gevent/100, NLP: prefork/8, Anal√≠ticas: prefork/4 |
| **Programaci√≥n** | Celery Beat con django-celery-beat (respaldado por base de datos) |

---

## 1. Investigaci√≥n de Granularidad de Tareas

### Hallazgo Clave: Coincidir Granularidad con Tipo de Tarea

**Procesamiento por Lotes (tareas vinculadas a I/O)**:
- Usar para recolectores de API: 25-50 elementos por lote
- Reduce sobrecarga de conexi√≥n en 40%
- Ideal cuando se espera I/O de red
- Biblioteca: celery-batches (flush_every o flush_interval)

**Procesamiento por Elemento √önico (tareas vinculadas a CPU)**:
- Usar para tareas NLP: 1 elemento por tarea
- Duraci√≥n variable (50ms a 5 segundos)
- Mejor distribuci√≥n de workers
- L√≥gica de reintento m√°s f√°cil

**M√©trica Clave**: Duraci√≥n de tarea
- Tareas > 100ms con duraci√≥n variable ‚Üí prefetch_multiplier=1
- Tareas < 100ms (cortas) ‚Üí prefetch_multiplier=50-150
- Configurar prefetch=1 para tareas largas reduce tiempo pendiente en 40%

### Recomendaci√≥n para el Sistema

**Recolectores**: Lote de 50 elementos
```python
@app.task(bind=True, max_retries=3, queue='youtube_collector')
def collect_youtube_batch(self, guideline_id, keywords, batch_size=50):
    # Llamadas API por lotes para eficiencia
    pass
```

**NLP**: Elemento √∫nico
```python
@app.task(bind=True, max_retries=2, queue='nlp_processing')
def process_nlp_single(self, content_id):
    # CPU-intensivo, duraci√≥n variable
    pass
```

**Anal√≠ticas**: Lote de 100-500 elementos
```python
@app.task(queue='analytics')
def aggregate_analytics_batch(content_ids):
    # Consultas de agregaci√≥n de base de datos
    pass
```

---

## 2. Investigaci√≥n de Organizaci√≥n de Colas

### Hallazgo Clave: Separar Colas por Tipo de Recurso

**Mejores Pr√°cticas**:
- Separar tareas vinculadas a I/O de vinculadas a CPU
- Usar colas de prioridad para alertas (priority=0 es m√°s alto con Redis)
- Configurar `broker_transport_options = {'queue_order_strategy': 'priority'}`
- Los workers pueden consumir m√∫ltiples colas: `-Q queue1,queue2,queue3`

### Recomendaci√≥n: 5 Colas

```python
task_queues = [
    Queue('youtube_collector'),    # Vinculado a I/O
    Queue('reddit_collector'),     # Vinculado a I/O
    Queue('mastodon_collector'),   # Vinculado a I/O
    Queue('nlp_processing'),       # Vinculado a CPU
    Queue('analytics'),            # Vinculado a DB
]
```

**Configuraci√≥n de Worker**:
```bash
# Recolectores (gevent para alta concurrencia de I/O)
celery -A app worker -Q youtube_collector,reddit_collector,mastodon_collector \
    --concurrency=100 --pool=gevent --prefetch-multiplier=4

# NLP (prefork para paralelismo de CPU)
celery -A app worker -Q nlp_processing \
    --concurrency=8 --pool=prefork --prefetch-multiplier=1

# Anal√≠ticas
celery -A app worker -Q analytics \
    --concurrency=4 --prefetch-multiplier=2
```

---

## 3. Investigaci√≥n de Patrones Canvas

### Hallazgo Clave: Usar Chord para Flujos de Trabajo Paralelo ‚Üí Secuencial

**Primitivas**:
- **Chain**: Secuencial (A ‚Üí B ‚Üí C)
- **Group**: Paralelo ([A, B, C])
- **Chord**: Paralelo + callback (group ‚Üí tarea √∫nica cuando todas completan)

**Requisitos de Chord**:
- Debe tener result_backend habilitado
- Las tareas NO deben tener ignore_result=True
- No soportado con backend RPC (usar Redis/DB)

### Recomendaci√≥n: Recolecci√≥n ‚Üí NLP ‚Üí Anal√≠ticas

```python
def daily_collection_workflow(guideline_id):
    # Paso 1: Recolectar de todas las plataformas en paralelo
    collection_tasks = group(
        collect_youtube_batch.s(...),
        collect_reddit_batch.s(...),
        collect_mastodon_batch.s(...),
    )

    # Paso 2: Encadenar a chord NLP
    workflow = chain(
        collection_tasks,           # Recolecci√≥n paralela
        flatten_content_ids.s(),    # Aplanar resultados
        create_nlp_chord.s(),       # Procesamiento NLP
    )

    return workflow.apply_async()

def create_nlp_chord(content_ids):
    # NLP paralelo, luego agregar cuando TODAS completen
    nlp_tasks = group(
        process_nlp_single.s(id) for id in content_ids
    )

    return chord(nlp_tasks)(
        aggregate_analytics_batch.s()
    )
```

**¬øPor qu√© chord en lugar de chain(group(), task)?**
- El callback de chord recibe TODOS los resultados del group
- Asegura que la agregaci√≥n solo se ejecute despu√©s de que TODAS las tareas NLP completen
- Cr√≠tico para anal√≠ticas que necesitan conjunto de datos completo

---

## 4. Investigaci√≥n de Manejo de Errores

### Hallazgo Clave: Estrategia de Manejo de Errores en Capas

**Capa 1: Reintentos Autom√°ticos**
- Retroceso exponencial: `countdown = 60 * (2 ** retries)`
- Reintentos m√°ximos: 3 para llamadas API, 2 para NLP
- Usar `autoretry_for=(ConnectionError, TimeoutError)`

**Capa 2: Circuit Breaker**
- Rastrear fallos por plataforma
- Abrir circuito despu√©s de 5 fallos en 5 minutos
- Previene fallos en cascada

**Capa 3: Dead Letter Queue**
- Respaldado por base de datos (Celery no tiene DLQ integrado)
- Almacenar: task_name, args, kwargs, error, traceback
- Interfaz de reintento manual

### Recomendaci√≥n

**Recolector con Reintento + Circuit Breaker**:
```python
@app.task(
    bind=True,
    max_retries=3,
    autoretry_for=(APIConnectionError,),
    retry_backoff=True,
    retry_backoff_max=600,
    acks_late=True,
)
def collect_youtube_batch(self, guideline_id, keywords, batch_size=50):
    circuit_breaker = CircuitBreaker('youtube')

    if circuit_breaker.is_open():
        logger.warning("Circuito ABIERTO - saltando")
        return []

    try:
        results = youtube_api.search(keywords)
        circuit_breaker.record_success()
        return results
    except Exception as exc:
        circuit_breaker.record_failure()
        raise
```

**Modelo DLQ de Base de Datos**:
```python
class FailedTask(models.Model):
    task_name = models.CharField(max_length=255)
    task_args = models.JSONField()
    exception_message = models.TextField()
    failed_at = models.DateTimeField(auto_now_add=True)
    retry_count = models.IntegerField(default=0)
    status = models.CharField(default='failed')
```

**Matriz de Manejo de Errores**:

| Tipo de Error | Estrategia | Reintentos M√°ximos | Retroceso |
|---------------|------------|-------------------|-----------|
| L√≠mite de Tasa API | Reintentar despu√©s de ventana | 3 | Fijo 60s |
| Cuota API Excedida | No reintentar, alertar | 0 | N/A |
| Error de Conexi√≥n | Retroceso exponencial | 3 | 60s, 120s, 240s |
| Error NLP | Reintento limitado | 2 | Fijo 60s |
| Error DB | Reintento r√°pido | 5 | 5s, 10s, 20s |

---

## 5. Investigaci√≥n de Configuraci√≥n de Rendimiento

### Hallazgo Clave: Ajustar Concurrencia y Prefetch Por Tipo de Worker

**Reglas de Concurrencia**:
- Vinculado a CPU: concurrency = n√∫cleos de CPU (ej., 8)
- Vinculado a I/O: concurrency = 2x-4x n√∫cleos O pool gevent (100+)
- Agregar >2x CPUs para tareas CPU degrada rendimiento

**Reglas de Prefetch**:
- Tareas largas/variables (>5s): prefetch=1 (reduce tiempo pendiente 40%)
- Tareas cortas (<100ms): prefetch=50-150
- Procesamiento por lotes: prefetch=2-4

**Backend de Resultados**:
- Redis: M√°s r√°pido (sub-ms), limitado por memoria
- PostgreSQL: M√°s lento, mejor persistencia
- Recomendaci√≥n: Redis con TTL de 1 d√≠a

### Recomendaci√≥n

**Configuraci√≥n de Celery**:
```python
# Broker & Backend de Resultados
broker_url = 'redis://localhost:6379/0'
result_backend = 'redis://localhost:6379/1'
result_expires = 86400  # 1 d√≠a

# Rendimiento
worker_prefetch_multiplier = 1  # Por defecto para tareas variables
worker_max_tasks_per_child = 100  # Prevenir fugas de memoria
worker_max_memory_per_child = 500000  # L√≠mite de 500MB

# Configuraci√≥n de tareas
task_acks_late = True  # Reconocer despu√©s de completar
task_reject_on_worker_lost = True  # Re-encolar en caso de crash
task_soft_time_limit = 300  # L√≠mite suave de 5 min
task_time_limit = 600  # L√≠mite duro de 10 min
```

**Comandos de Worker**:
```bash
# Recolectores (I/O: gevent, alta concurrencia)
celery -A app worker -Q youtube_collector,reddit_collector,mastodon_collector \
    --concurrency=100 --pool=gevent --prefetch-multiplier=4

# NLP (CPU: prefork, coincidir con n√∫cleos)
celery -A app worker -Q nlp_processing \
    --concurrency=8 --pool=prefork --prefetch-multiplier=1 \
    --max-memory-per-child=500000

# Anal√≠ticas (DB: prefork, moderado)
celery -A app worker -Q analytics \
    --concurrency=4 --prefetch-multiplier=2
```

---

## 6. Investigaci√≥n de Programaci√≥n

### Hallazgo Clave: Usar Programador Beat Respaldado por Base de Datos

**Celery Beat**:
- Env√≠a tareas a intervalos a workers
- **CR√çTICO**: Solo ejecutar UNA instancia de beat (previene duplicados)
- Por defecto: archivo local (celerybeat-schedule) - no seguro para multi-servidor
- Producci√≥n: django-celery-beat (respaldado por base de datos, programaciones din√°micas)

**Tipos de Programaci√≥n**:
- Crontab: `crontab(hour=2, minute=0)` (diario 2 AM)
- Interval: `timedelta(hours=6)` (cada 6 horas)
- Solar: `solar('sunset', lat, lon)` (astron√≥mico)

### Recomendaci√≥n

**Programaci√≥n de Beat**:
```python
from celery.schedules import crontab
from datetime import timedelta

beat_schedule = {
    'collect-all-guidelines': {
        'task': 'collectors.collect_all_active_guidelines',
        'schedule': timedelta(hours=6),  # Cada 6 horas
        'options': {'queue': 'youtube_collector'}
    },
    'detect-trending': {
        'task': 'analytics.detect_trending',
        'schedule': timedelta(minutes=30),  # Cada 30 min
        'options': {'queue': 'analytics'}
    },
    'validate-trends': {
        'task': 'validation.validate_with_google_trends',
        'schedule': crontab(hour=2, minute=0),  # Diario 2 AM
        'options': {'queue': 'analytics'}
    },
    'cleanup-old-data': {
        'task': 'maintenance.cleanup_old_data',
        'schedule': crontab(hour=3, minute=0),  # Diario 3 AM
        'options': {'queue': 'analytics'}
    },
}

beat_scheduler = 'django_celery_beat.schedulers:DatabaseScheduler'
```

**Ejecutar Beat**:
```bash
# Servicio separado (PRODUCCI√ìN)
celery -A app beat --scheduler django_celery_beat.schedulers:DatabaseScheduler

# ¬°NUNCA ejecutar m√∫ltiples instancias de beat!
```

---

## 7. Resumen de Arquitectura Completa

### Diagrama de Flujo de Trabajo

```
Celery Beat (Cada 6 horas)
    ‚Üì
collect_all_active_guidelines
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  GROUP: Recolecci√≥n Paralela      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ YouTube  ‚îÇ  ‚îÇ Reddit   ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ (lote    ‚îÇ  ‚îÇ (lote    ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  50)     ‚îÇ  ‚îÇ  50)     ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ       ‚Üì              ‚Üì            ‚îÇ
‚îÇ   [yt_ids]      [rd_ids]         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
flatten_content_ids
    ‚Üì
[id1, id2, ..., id100]
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CHORD: NLP Paralelo              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇNLP 1‚îÇ ‚îÇNLP 2‚îÇ ‚îÇNLP 3‚îÇ ...     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ  (1 contenido por tarea)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì (esperar a que TODAS completen)
aggregate_analytics_batch([id1...id100])
    ‚Üì
Verificar tendencias ‚Üí send_alert si >50% de crecimiento
```

### Resumen de Configuraci√≥n

| Componente | Configuraci√≥n | Justificaci√≥n |
|------------|--------------|---------------|
| **Recolectores** | Lote 50, gevent/100, prefetch=4 | Optimizaci√≥n de I/O |
| **NLP** | Elemento √∫nico, prefork/8, prefetch=1 | Equidad de CPU |
| **Anal√≠ticas** | Lote, prefork/4, prefetch=2 | Eficiencia de DB |
| **Colas** | 5 colas separadas | Aislamiento |
| **Canvas** | chain(group, chord) | Paralelo ‚Üí Secuencial |
| **Reintento** | 3x exponencial + DLQ | Resiliencia |
| **Backend** | Redis TTL 1 d√≠a | Velocidad + limpieza |
| **Beat** | Respaldado por base de datos | Programaciones din√°micas |

### Objetivos de Rendimiento

- **Recolecci√≥n**: 4,000 elementos/d√≠a √∑ 24 horas = 167 elementos/hora
  - 3 plataformas √ó 50 lote = 150 elementos por ejecuci√≥n de recolecci√≥n
  - Cada 6 horas = 600 elementos/d√≠a por plataforma ‚Üí 1,800/d√≠a total ‚úÖ

- **NLP**: 8 n√∫cleos √ó 20 elementos/hora = 160 elementos/hora ‚úÖ

- **Retraso de Cola**: <5 minutos (prefetch=1 previene acaparamiento)

---

## Ejemplos de Definiciones de Tareas

**Recolector**:
```python
@app.task(bind=True, max_retries=3, queue='youtube_collector')
def collect_youtube_batch(self, guideline_id, keywords, batch_size=50):
    results = []
    for keyword in keywords:
        items = youtube_api.search(q=keyword, maxResults=batch_size)
        results.extend(items)
    return [ContentRecolectado.create(item).id for item in results]
```

**NLP**:
```python
@app.task(bind=True, max_retries=2, queue='nlp_processing')
def process_nlp_single(self, content_id):
    content = ContentRecolectado.objects.get(id=content_id)
    content.topics = topic_modeling.identify(content.text)
    content.sentiment = sentiment_analyzer.analyze(content.text)
    content.save()
    return content_id
```

**Anal√≠ticas**:
```python
@app.task(queue='analytics')
def aggregate_analytics_batch(content_ids):
    stats = calculate_demographics(content_ids)
    trends = identify_trending(content_ids)
    Demografia.bulk_upsert(stats)
    Tendencia.bulk_upsert(trends)
```

**Orquestador**:
```python
@app.task
def daily_collection_workflow(guideline_id):
    guideline = Lineamiento.objects.get(id=guideline_id)

    collection = group(
        collect_youtube_batch.s(guideline_id, guideline.keywords, 50),
        collect_reddit_batch.s(guideline_id, guideline.keywords, 50),
    )

    workflow = chain(
        collection,
        flatten_content_ids.s(),
        create_nlp_chord.s(),
    )

    return workflow.apply_async()
```

---

## Referencias

### Documentaci√≥n Oficial
- [Celery Canvas Patterns](https://docs.celeryq.dev/en/stable/userguide/canvas.html)
- [Celery Optimizing](https://docs.celeryq.dev/en/stable/userguide/optimizing.html)
- [Celery Routing](https://docs.celeryq.dev/en/stable/userguide/routing.html)
- [Celery Periodic Tasks](https://docs.celeryq.dev/en/stable/userguide/periodic-tasks.html)

### Bibliotecas
- [celery-batches](https://github.com/clokep/celery-batches) - Procesamiento por lotes
- [django-celery-beat](https://github.com/celery/django-celery-beat) - Programador DB
- [flower](https://github.com/mher/flower) - Dashboard de monitoreo

### Mejores Pr√°cticas
- [Celery Task Routing & Error Handling](https://usmanasifbutt.github.io/blog/2025/03/13/celery-task-routing-and-retries.html)
- [Mastering Task Orchestration](https://medium.com/@mortezasaki91/mastering-task-orchestration-with-celery-exploring-groups-chains-and-chords-991f9e407a4f)
- [Configuration Best Practices](https://moldstud.com/articles/p-celery-configuration-best-practices-enhance-your-task-queue-efficiency)

---

## Estado: ‚úÖ **INVESTIGACI√ìN DE CELERY COMPLETA**

**Respuestas a Preguntas de Investigaci√≥n**:

1. ‚úÖ **Granularidad de Tarea**: Recolectores por lotes (50), NLP √∫nico, anal√≠ticas por lotes
2. ‚úÖ **Organizaci√≥n de Colas**: 5 colas (plataforma + nlp + anal√≠ticas)
3. ‚úÖ **Patrones Canvas**: chain(group, chord) para recolecci√≥n‚ÜíNLP‚Üíanal√≠ticas
4. ‚úÖ **Manejo de Errores**: Retroceso exponencial + DLQ + circuit breaker
5. ‚úÖ **Rendimiento**: Ajuste de prefetch, backend Redis, gevent para I/O
6. ‚úÖ **Programaci√≥n**: django-celery-beat con programaciones crontab + interval

**Siguiente Fase**: Implementar estructura de tareas en Fase 2 (ejecuci√≥n de tareas)
