# TrendsGPX Backend - Configuraci√≥n de Claude Code

## üéØ Resumen del Proyecto

**TrendsGPX Backend** es un sistema de an√°lisis de tendencias en redes sociales que identifica, analiza y reporta temas trending con segmentaci√≥n demogr√°fica detallada (Plataforma ‚Üí Ubicaci√≥n ‚Üí Edad ‚Üí G√©nero).

### Stack Tecnol√≥gico
- **Framework**: FastAPI 0.104+
- **Base de Datos**: PostgreSQL 15+ con TimescaleDB
- **Queue**: Celery 5.3+ con Redis
- **NLP**: spaCy, BERTopic, pysentimiento, RoBERTuito
- **APIs**: YouTube Data API v3, Reddit API (PRAW), Mastodon API, Google Trends

### Caracter√≠sticas Clave
- ‚úÖ Recolecci√≥n multi-plataforma (YouTube, Reddit, Mastodon)
- ‚úÖ Procesamiento NLP en espa√±ol (Colombia, M√©xico, Argentina)
- ‚úÖ Segmentaci√≥n demogr√°fica 4 niveles
- ‚úÖ An√°lisis de tendencias con TimescaleDB
- ‚úÖ API REST completa con 14 endpoints
- ‚úÖ Tareas as√≠ncronas con Celery (4 colas especializadas)

## üìÅ Estructura del Proyecto

```
backend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api/              # FastAPI application
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py       # App principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth.py       # Autenticaci√≥n API key
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes/       # Endpoints (lineamientos, collector, tendencias)
‚îÇ   ‚îú‚îÄ‚îÄ models/           # SQLAlchemy ORM models (6 entidades)
‚îÇ   ‚îú‚îÄ‚îÄ schemas/          # Pydantic schemas
‚îÇ   ‚îú‚îÄ‚îÄ services/         # L√≥gica de negocio
‚îÇ   ‚îú‚îÄ‚îÄ collectors/       # Recolectores (YouTube, Reddit, Mastodon)
‚îÇ   ‚îú‚îÄ‚îÄ nlp/              # Servicios NLP (spaCy, sentiment, topics)
‚îÇ   ‚îú‚îÄ‚îÄ tasks/            # Tareas Celery (collector, nlp, analytics)
‚îÇ   ‚îú‚îÄ‚îÄ utils/            # Utilidades (config, logging, rate_limiter)
‚îÇ   ‚îî‚îÄ‚îÄ celery_app.py     # Configuraci√≥n Celery
‚îú‚îÄ‚îÄ alembic/              # Migraciones de base de datos (10 migraciones)
‚îú‚îÄ‚îÄ tests/                # Tests con pytest
‚îú‚îÄ‚îÄ docs/                 # Documentaci√≥n Speckit
‚îÇ   ‚îú‚îÄ‚îÄ speckit/          # Artefactos de dise√±o
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spec.md       # Especificaci√≥n funcional (28 KB)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ plan.md       # Plan de implementaci√≥n (14 KB)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tasks.md      # 148 tareas (100% completadas)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data-model.md # Modelo de datos (23 KB)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ research.md   # Investigaci√≥n de APIs (77 KB)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nlp-research.md # Investigaci√≥n NLP (40 KB)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ quickstart.md # Gu√≠a de inicio r√°pido
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ contracts/    # OpenAPI y eventos Celery
‚îÇ   ‚îî‚îÄ‚îÄ README.md         # √çndice de documentaci√≥n
‚îú‚îÄ‚îÄ docker-compose.yml    # Stack completo (postgres, redis, api, celery)
‚îú‚îÄ‚îÄ pyproject.toml        # Dependencias con Poetry
‚îî‚îÄ‚îÄ README.md             # README principal del proyecto
```

## üß† Metodolog√≠a Speckit

Este proyecto fue construido usando **Speckit**, una metodolog√≠a estructurada de desarrollo:

### Comandos Disponibles
- `/speckit.specify` - Crear/actualizar especificaci√≥n funcional
- `/speckit.plan` - Dise√±ar arquitectura t√©cnica
- `/speckit.tasks` - Generar lista de tareas implementables
- `/speckit.implement` - Ejecutar implementaci√≥n
- `/speckit.analyze` - Verificar consistencia entre artefactos
- `/speckit.clarify` - Identificar √°reas poco especificadas
- `/speckit.checklist` - Generar checklists de validaci√≥n

### Documentaci√≥n Clave
1. **docs/speckit/spec.md** - Especificaci√≥n funcional con 9 user stories
2. **docs/speckit/plan.md** - Plan t√©cnico y decisiones de arquitectura
3. **docs/speckit/tasks.md** - 148 tareas organizadas en 5 fases
4. **docs/speckit/data-model.md** - Esquema de base de datos completo
5. **docs/speckit/research.md** - Investigaci√≥n de APIs de plataformas
6. **docs/speckit/nlp-research.md** - Investigaci√≥n de modelos NLP para espa√±ol

## üîß Configuraci√≥n del Entorno

### Variables de Entorno Requeridas (.env)
```bash
# Database
DATABASE_URL=postgresql://user:password@localhost:5432/trendsgpx
POSTGRES_USER=trendsgpx_user
POSTGRES_PASSWORD=secure_password
POSTGRES_DB=trendsgpx

# Redis
REDIS_URL=redis://localhost:6379/0

# API Keys
YOUTUBE_API_KEY=your_youtube_api_key
REDDIT_CLIENT_ID=your_reddit_client_id
REDDIT_CLIENT_SECRET=your_reddit_client_secret
MASTODON_ACCESS_TOKEN=your_mastodon_token
MASTODON_API_BASE_URL=https://mastodon.social

# App Config
API_KEY=dev-api-key-change-in-production
LOG_LEVEL=INFO
CORS_ORIGINS=["http://localhost:3000"]

# Rate Limiting
YOUTUBE_QUOTA_LIMIT=10000
REDDIT_RATE_LIMIT=60
MASTODON_RATE_LIMIT=300

# Trending Thresholds
MIN_MENTIONS_FOR_TREND=10
GROWTH_RATE_THRESHOLD=0.5
DATA_RETENTION_DAYS=7
```

## üöÄ Comandos √ötiles

### Desarrollo Local
```bash
# Instalar dependencias
poetry install

# Descargar modelo spaCy
python -m spacy download es_core_news_md

# Ejecutar migraciones
alembic upgrade head

# Iniciar API
uvicorn src.api.main:app --reload --host 0.0.0.0 --port 8000

# Iniciar Celery worker
celery -A src.celery_app worker --loglevel=info -Q collectors,nlp,analytics

# Iniciar Celery beat (tareas programadas)
celery -A src.celery_app beat --loglevel=info

# Monitorear Celery (Flower)
celery -A src.celery_app flower --port=5555
```

### Con Docker
```bash
# Iniciar todo el stack
docker-compose up -d

# Ver logs
docker-compose logs -f api
docker-compose logs -f celery_worker

# Ejecutar migraciones
docker-compose exec api alembic upgrade head

# Detener todo
docker-compose down
```

### Testing
```bash
# Ejecutar todos los tests
pytest

# Con cobertura
pytest --cov=src --cov-report=html

# Solo tests de lineamientos
pytest tests/test_lineamientos.py -v
```

## üìä Modelo de Datos

### 6 Entidades Principales

1. **lineamientos** - Configuraci√≥n de b√∫squeda
   - keywords (JSONB), plataformas (JSONB), activo (Boolean)

2. **contenido_recolectado** - Contenido de redes sociales
   - plataforma, plataforma_id, contenido_texto, metadata (JSONB)
   - √çndice full-text search en espa√±ol

3. **temas_identificados** - Temas extra√≠dos con NLP
   - tema_nombre, keywords (JSONB), probabilidad

4. **demografia** - Segmentaci√≥n demogr√°fica
   - ubicacion, edad_rango, genero, inferido (Boolean)

5. **tendencias** - Serie temporal de tendencias
   - Hypertable de TimescaleDB (particionado por fecha_hora)
   - Composite PRIMARY KEY (fecha_hora, tema_id, plataforma, ubicacion, edad_rango, genero)
   - Continuous aggregates para agregaciones horarias/diarias

6. **validacion_tendencias** - Validaci√≥n con Google Trends
   - volumen_busquedas, correlacion

## üîÑ Flujo de Trabajo

### 1. Crear Lineamiento
```bash
POST /lineamientos/
{
  "nombre": "Tecnolog√≠a IA",
  "keywords": ["IA", "inteligencia artificial"],
  "plataformas": ["youtube", "reddit"]
}
```

### 2. Recolectar Contenido (Autom√°tico cada 30 min)
```bash
POST /collect/lineamiento/{id}?hours_back=24
```
‚Üí Dispara tareas Celery en cola `collectors`

### 3. Procesamiento NLP (Autom√°tico cada hora)
- Tarea `process_pending_content` en cola `nlp`
- Extrae entidades, keywords, sentimiento
- Crea registros en `temas_identificados` y `demografia`

### 4. An√°lisis de Tendencias (Autom√°tico cada hora)
- Tarea `analyze_trends` en cola `analytics`
- Calcula volumen, crecimiento, marca como tendencia
- Inserta en hypertable `tendencias`

### 5. Validaci√≥n con Google Trends (Cada 6 horas)
- Tarea `validate_trends` en cola `validation`
- Correlaciona con volumen de b√∫squedas

### 6. Consultar Tendencias
```bash
GET /tendencias/?hours_back=24&solo_activas=true
GET /tendencias/agregadas?top_n=10
GET /tendencias/jerarquicas?hours_back=24
```

## üé® Patrones de Dise√±o

### Services Pattern
Toda la l√≥gica de negocio est√° en `src/services/`:
- `lineamiento_service.py` - CRUD de lineamientos
- `content_service.py` - Gesti√≥n de contenido recolectado
- `trends_service.py` - Queries de tendencias

### Collectors Pattern
Cada plataforma tiene su propio collector en `src/collectors/`:
- `youtube_collector.py` - YouTube Data API v3
- `reddit_collector.py` - Reddit API con PRAW
- `mastodon_collector.py` - Mastodon API

### Rate Limiting
`src/utils/rate_limiter.py` implementa Token Bucket Algorithm:
```python
rate_limiter = RateLimiterManager.get_limiter("youtube", max_requests=100, time_window=60)
rate_limiter.acquire()  # Bloquea hasta que haya tokens disponibles
```

### Celery Canvas
Orquestaci√≥n de tareas con group, chain, chord:
```python
# Recolecci√≥n paralela
job = group([
    collect_youtube.s(lineamiento_id, keywords),
    collect_reddit.s(lineamiento_id, keywords),
])
result = job.apply_async()
```

## üß™ Testing

### Configuraci√≥n de Tests
- `tests/conftest.py` - Fixtures compartidas
- SQLite in-memory para tests
- Override de dependencias (get_db)

### Cobertura Actual
- 18 tests en `test_lineamientos.py`
- Cubre full CRUD + soft delete + activaci√≥n
- Tests de validaci√≥n (401, 403, 404, 422, 400)

## üìö Documentaci√≥n de API

### Endpoints Principales

**Lineamientos**
- `POST /lineamientos/` - Crear (201)
- `GET /lineamientos/` - Listar con paginaci√≥n
- `GET /lineamientos/{id}` - Obtener por ID
- `PUT /lineamientos/{id}` - Actualizar
- `DELETE /lineamientos/{id}` - Soft delete (204)
- `POST /lineamientos/{id}/activate` - Reactivar

**Recolecci√≥n**
- `POST /collect/lineamiento/{id}` - Recolectar todas las plataformas (202)
- `POST /collect/lineamiento/{id}/platform/{platform}` - Plataforma espec√≠fica
- `POST /collect/all` - Todos los lineamientos activos
- `GET /collect/task/{task_id}` - Estado de tarea

**Tendencias**
- `GET /tendencias/` - Lista con filtros (plataforma, ubicacion, solo_activas)
- `GET /tendencias/agregadas` - Agregadas por tema cross-platform
- `GET /tendencias/jerarquicas` - Estructura jer√°rquica 4 niveles

### Documentaci√≥n Interactiva
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`
- OpenAPI spec: `http://localhost:8000/openapi.json`

## üîí Seguridad

### Autenticaci√≥n
- API Key en header `X-API-Key`
- Middleware de autenticaci√≥n en `src/api/auth.py`
- Endpoints p√∫blicos: `/health`, `/`, `/docs`, `/redoc`

### Configuraci√≥n
```python
# src/api/auth.py
async def get_api_key(x_api_key: str = Header(...)):
    if x_api_key != settings.api_key:
        raise HTTPException(status_code=403, detail="API key inv√°lida")
    return x_api_key
```

## üêõ Troubleshooting

### Error: Celery no conecta a Redis
```bash
# Verificar que Redis est√° corriendo
docker-compose ps redis
# Verificar REDIS_URL en .env
echo $REDIS_URL
```

### Error: Migraciones fallan
```bash
# Revisar estado de migraciones
alembic current
# Rollback si es necesario
alembic downgrade -1
# Aplicar nuevamente
alembic upgrade head
```

### Error: spaCy modelo no encontrado
```bash
# Descargar modelo
python -m spacy download es_core_news_md
# O usar Docker
docker-compose exec api python -m spacy download es_core_news_md
```

### Error: YouTube API quota exceeded
```bash
# Verificar uso de cuota en Google Cloud Console
# Ajustar YOUTUBE_QUOTA_LIMIT en .env
# La cuota se reinicia diariamente (Pacific Time)
```

## üìù Convenciones del C√≥digo

### Naming
- **Archivos**: snake_case (e.g., `lineamiento_service.py`)
- **Clases**: PascalCase (e.g., `LineamientoCreate`)
- **Funciones**: snake_case (e.g., `create_lineamiento`)
- **Constantes**: UPPER_SNAKE_CASE (e.g., `MAX_KEYWORDS`)

### Docstrings
```python
def analyze_trends(lineamiento_id: UUID) -> List[Tendencia]:
    """
    Analiza tendencias para un lineamiento espec√≠fico.

    Args:
        lineamiento_id: UUID del lineamiento a analizar

    Returns:
        Lista de tendencias detectadas

    Raises:
        ValueError: Si el lineamiento no existe
    """
```

### Type Hints
- Siempre usar type hints en funciones
- Usar `from typing import List, Dict, Optional, Union`
- Usar Pydantic models para validaci√≥n

### Logging
```python
import logging
logger = logging.getLogger(__name__)

logger.info(f"Recolectando contenido para lineamiento {lineamiento_id}")
logger.warning(f"Rate limit alcanzado para YouTube")
logger.error(f"Error al procesar contenido: {error}", exc_info=True)
```

## üéØ Pr√≥ximos Pasos

### Features Pendientes (seg√∫n spec.md)
- [ ] Implementar an√°lisis de sentiment profundo
- [ ] Agregar soporte para TikTok Creative Center
- [ ] Implementar predicci√≥n de tendencias con ML
- [ ] Dashboard web con visualizaciones
- [ ] Sistema de notificaciones en tiempo real
- [ ] Soporte multi-idioma (adem√°s de espa√±ol)

### Mejoras T√©cnicas
- [ ] A√±adir tests para collectors
- [ ] Implementar cache con Redis
- [ ] Optimizar queries con √≠ndices adicionales
- [ ] A√±adir monitoring con Prometheus/Grafana
- [ ] Implementar CI/CD con GitHub Actions
- [ ] A√±adir documentaci√≥n de arquitectura (diagramas)

## üìû Contacto y Recursos

### Repositorio
- **GitHub**: git@github.com:juansuarez-pragma/trendsgpx.git
- **Autor**: Juan Suarez (@juansuarez-pragma)

### Referencias Externas
- [FastAPI Docs](https://fastapi.tiangolo.com/)
- [TimescaleDB Docs](https://docs.timescale.com/)
- [Celery Docs](https://docs.celeryq.dev/)
- [spaCy Docs](https://spacy.io/)
- [BERTopic Docs](https://maartengr.github.io/BERTopic/)

### Documentaci√≥n Interna
- `docs/README.md` - √çndice completo de documentaci√≥n
- `API_DOCUMENTATION.md` - Gu√≠a completa de API con ejemplos
- `docs/speckit/quickstart.md` - Gu√≠a de inicio r√°pido

---

**Generado con**: Speckit + Claude Code
**Fecha**: Noviembre 2025
**Versi√≥n del Proyecto**: 1.0.0
