# TrendsGPX Backend ğŸš€

<div align="center">

![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15+-blue.svg)
![TimescaleDB](https://img.shields.io/badge/TimescaleDB-enabled-orange.svg)
![Speckit](https://img.shields.io/badge/docs-Speckit-blue.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)

**Sistema de AnÃ¡lisis de Tendencias en Redes Sociales con SegmentaciÃ³n DemogrÃ¡fica y NLP**

[CaracterÃ­sticas](#caracterÃ­sticas) â€¢
[InstalaciÃ³n](#instalaciÃ³n) â€¢
[API](#documentaciÃ³n-de-api) â€¢
[Arquitectura](#arquitectura) â€¢
[Contribuir](#contribuir)

</div>

---

## ğŸ“‹ Tabla de Contenidos

- [CaracterÃ­sticas](#caracterÃ­sticas)
- [DocumentaciÃ³n](#documentaciÃ³n)
- [Requisitos](#requisitos)
- [InstalaciÃ³n](#instalaciÃ³n)
  - [InstalaciÃ³n con Docker](#instalaciÃ³n-con-docker-recomendado)
  - [InstalaciÃ³n Manual](#instalaciÃ³n-manual)
- [ConfiguraciÃ³n](#configuraciÃ³n)
- [Uso](#uso)
- [DocumentaciÃ³n de API](#documentaciÃ³n-de-api)
- [Arquitectura](#arquitectura)
- [Stack TecnolÃ³gico](#stack-tecnolÃ³gico)
- [Tareas Programadas](#tareas-programadas)
- [Testing](#testing)
- [Despliegue](#despliegue)
- [Contribuir](#contribuir)
- [Licencia](#licencia)

## âœ¨ CaracterÃ­sticas

### ğŸ”„ RecolecciÃ³n AutomÃ¡tica de Contenido
- **YouTube**: Videos, comentarios, estadÃ­sticas (views, likes, comments)
- **Reddit**: Posts, comentarios, scoring, subreddits en espaÃ±ol
- **Mastodon**: Toots, contexto, hashtags trending
- Rate limiting automÃ¡tico por plataforma
- DeduplicaciÃ³n de contenido

### ğŸ¤– AnÃ¡lisis NLP Avanzado para EspaÃ±ol
- **spaCy** (es_core_news_md): Named Entity Recognition (NER), extracciÃ³n de keywords
- **RoBERTuito/BERTopic**: Topic modeling con embeddings en espaÃ±ol
- **pysentimiento**: AnÃ¡lisis de sentimiento (positivo/neutro/negativo)
- Procesamiento batch optimizado

### ğŸ“Š SegmentaciÃ³n DemogrÃ¡fica (4 Niveles)
```
Plataforma â†’ UbicaciÃ³n (PaÃ­s/Ciudad) â†’ Edad â†’ GÃ©nero
```
- DetecciÃ³n automÃ¡tica de ubicaciÃ³n desde texto y metadata
- Clustering demogrÃ¡fico inteligente
- Queries jerÃ¡rquicas optimizadas

### âœ… ValidaciÃ³n de Tendencias
- IntegraciÃ³n con **Google Trends** (pytrends)
- ValidaciÃ³n cruzada de popularidad
- MÃ©tricas de confianza

### ğŸ“ ExportaciÃ³n Multi-Formato
- JSON (estructurado)
- CSV (anÃ¡lisis)
- Excel (reportes)

### âš¡ Procesamiento AsÃ­ncrono
- **Celery** con 5 colas especializadas:
  - `collectors`: RecolecciÃ³n de contenido
  - `nlp`: Procesamiento NLP
  - `analytics`: AnÃ¡lisis de tendencias
  - `validation`: ValidaciÃ³n con Google Trends
  - `default`: Tareas generales
- Workers escalables horizontalmente
- Retry automÃ¡tico con exponential backoff

### ğŸ’¾ Base de Datos Optimizada
- **PostgreSQL 15** + **TimescaleDB** para series temporales
- Hypertables con chunks de 7 dÃ­as
- Continuous aggregates (hora/dÃ­a)
- Retention policy automÃ¡tica
- Full-text search en espaÃ±ol

## ğŸ“š DocumentaciÃ³n

Este proyecto incluye documentaciÃ³n completa desarrollada con la metodologÃ­a **Speckit**.

### ğŸ“– DocumentaciÃ³n Principal

| Documento | DescripciÃ³n | TamaÃ±o |
|-----------|-------------|--------|
| [ğŸ“‹ Ãndice de DocumentaciÃ³n](docs/README.md) | GuÃ­a completa de toda la documentaciÃ³n | - |
| [ğŸ“ EspecificaciÃ³n Funcional](docs/speckit/spec.md) | User stories, requisitos y criterios de aceptaciÃ³n | 28 KB |
| [ğŸ—ï¸ Plan de ImplementaciÃ³n](docs/speckit/plan.md) | Arquitectura tÃ©cnica y stack tecnolÃ³gico | 14 KB |
| [âœ… Lista de Tareas](docs/speckit/tasks.md) | 148 tareas organizadas en 5 fases (100% completadas) | 39 KB |
| [ğŸ—„ï¸ Modelo de Datos](docs/speckit/data-model.md) | Esquema de base de datos completo | 23 KB |
| [ğŸ”¬ InvestigaciÃ³n TÃ©cnica](docs/speckit/research.md) | AnÃ¡lisis de APIs, bibliotecas y arquitectura | 77 KB |
| [ğŸ¤– InvestigaciÃ³n NLP](docs/speckit/nlp-research.md) | Deep dive en NLP para espaÃ±ol | 40 KB |
| [ğŸš€ GuÃ­a de Inicio RÃ¡pido](docs/speckit/quickstart.md) | Setup para desarrolladores | 16 KB |

### ğŸ“„ Contratos de API

| Documento | DescripciÃ³n |
|-----------|-------------|
| [OpenAPI Specification](docs/speckit/contracts/openapi.yaml) | 14 endpoints REST documentados |
| [Celery Events](docs/speckit/contracts/events.yaml) | 11 tareas asÃ­ncronas especificadas |

### ğŸ”§ MetodologÃ­a Speckit

Este proyecto fue construido usando **Speckit**, una metodologÃ­a estructurada que garantiza:

- âœ… **Trazabilidad completa**: Desde requisitos hasta cÃ³digo
- âœ… **DocumentaciÃ³n actualizada**: Sincronizada con la implementaciÃ³n
- âœ… **Reproducibilidad**: El proyecto puede reconstruirse desde los docs
- âœ… **Onboarding rÃ¡pido**: Nuevos desarrolladores pueden entender el sistema completo

**Comandos Speckit disponibles** (ver [docs/.claude/commands/](docs/.claude/commands/)):
- `/speckit.specify` - Crear/actualizar especificaciÃ³n
- `/speckit.plan` - DiseÃ±ar arquitectura
- `/speckit.tasks` - Generar lista de tareas
- `/speckit.implement` - Ejecutar implementaciÃ³n
- `/speckit.analyze` - Verificar consistencia

### ğŸ“Š EstadÃ­sticas de DocumentaciÃ³n

```
ğŸ“¦ 31 archivos de documentaciÃ³n
ğŸ“ ~240,000 caracteres
âœ… 148 tareas (100% completadas)
ğŸ¯ 5 fases implementadas
ğŸ“– 2 contratos de API (OpenAPI + Events)
```

## ğŸ“¦ Requisitos

### Requisitos de Sistema
- Python 3.11 o superior
- PostgreSQL 15+ con extensiÃ³n TimescaleDB
- Redis 6.0+
- 2GB RAM mÃ­nimo (4GB recomendado)
- Docker & Docker Compose (para instalaciÃ³n rÃ¡pida)

### API Keys Requeridas
- **YouTube Data API v3**: [Obtener aquÃ­](https://console.cloud.google.com/)
- **Reddit API**: [Obtener aquÃ­](https://www.reddit.com/prefs/apps)
- **Mastodon API**: Obtener en tu instancia â†’ Preferencias â†’ Development

## ğŸš€ InstalaciÃ³n

### InstalaciÃ³n con Docker (Recomendado)

#### 1. Clonar el repositorio
```bash
git clone https://github.com/tu-usuario/trendsgpx-backend.git
cd trendsgpx-backend
```

#### 2. Configurar variables de entorno
```bash
cp .env.example .env
nano .env  # o tu editor preferido
```

Edita `.env` con tus API keys:
```env
# Database
DATABASE_URL=postgresql://trendsgpx:password@postgres:5432/trendsgpx

# Redis
REDIS_URL=redis://redis:6379/0

# API Keys - Plataformas
YOUTUBE_API_KEY=tu_youtube_api_key_aqui
REDDIT_CLIENT_ID=tu_reddit_client_id
REDDIT_CLIENT_SECRET=tu_reddit_client_secret
MASTODON_ACCESS_TOKEN=tu_mastodon_access_token

# Security
API_KEY=cambia-esto-en-produccion

# ConfiguraciÃ³n
LOG_LEVEL=INFO
```

#### 3. Iniciar servicios con Docker Compose
```bash
docker-compose up -d
```

Esto inicia automÃ¡ticamente:
- âœ… PostgreSQL 15 + TimescaleDB (puerto 5432)
- âœ… Redis (puerto 6379)
- âœ… FastAPI Backend (puerto 8000)
- âœ… Celery Workers (collectors, NLP, analytics)
- âœ… Celery Beat (scheduler)
- âœ… Flower (monitoring - puerto 5555)

#### 4. Ejecutar migraciones
```bash
docker-compose exec api alembic upgrade head
```

#### 5. Verificar instalaciÃ³n
```bash
curl http://localhost:8000/health
```

Respuesta esperada:
```json
{
  "status": "healthy",
  "version": "1.0.0",
  "service": "TrendsGPX API"
}
```

### InstalaciÃ³n Manual

#### 1. Instalar dependencias del sistema
```bash
# macOS
brew install postgresql@15 redis

# Ubuntu/Debian
sudo apt-get update
sudo apt-get install postgresql-15 postgresql-15-timescaledb redis-server

# Habilitar TimescaleDB
sudo -u postgres psql -c "CREATE EXTENSION IF NOT EXISTS timescaledb CASCADE;"
```

#### 2. Instalar Poetry
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

#### 3. Instalar dependencias de Python
```bash
poetry install
```

#### 4. Descargar modelo de spaCy
```bash
poetry run python -m spacy download es_core_news_md
```

#### 5. Configurar variables de entorno
```bash
cp .env.example .env
# Editar .env con tus configuraciones
```

#### 6. Ejecutar migraciones
```bash
poetry run alembic upgrade head
```

#### 7. Iniciar servicios

**Terminal 1 - FastAPI:**
```bash
poetry run uvicorn src.api.main:app --reload --host 0.0.0.0 --port 8000
```

**Terminal 2 - Celery Worker (Collectors):**
```bash
poetry run celery -A src.celery_app worker -Q collectors -l info
```

**Terminal 3 - Celery Worker (NLP):**
```bash
poetry run celery -A src.celery_app worker -Q nlp -l info
```

**Terminal 4 - Celery Worker (Analytics):**
```bash
poetry run celery -A src.celery_app worker -Q analytics -l info
```

**Terminal 5 - Celery Beat:**
```bash
poetry run celery -A src.celery_app beat -l info
```

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno Principales

| Variable | DescripciÃ³n | Default |
|----------|-------------|---------|
| `DATABASE_URL` | URL de PostgreSQL | `postgresql://trendsgpx:password@localhost:5432/trendsgpx` |
| `REDIS_URL` | URL de Redis | `redis://localhost:6379/0` |
| `API_KEY` | API key para autenticaciÃ³n | `dev-api-key-change-in-production` |
| `YOUTUBE_API_KEY` | API key de YouTube | - |
| `REDDIT_CLIENT_ID` | Client ID de Reddit | - |
| `REDDIT_CLIENT_SECRET` | Client Secret de Reddit | - |
| `MASTODON_ACCESS_TOKEN` | Access token de Mastodon | - |
| `LOG_LEVEL` | Nivel de logging | `INFO` |
| `TRENDING_GROWTH_THRESHOLD` | Umbral de crecimiento (0.5 = 50%) | `0.5` |
| `TRENDING_MIN_MENTIONS` | MÃ­nimo de menciones para tendencia | `10` |
| `DATA_RETENTION_DAYS` | DÃ­as de retenciÃ³n de datos | `7` |

Ver `.env.example` para la lista completa.

### Rate Limiting por Plataforma

| Plataforma | LÃ­mite | PerÃ­odo |
|------------|--------|---------|
| YouTube | 10,000 unidades | 1 dÃ­a |
| Reddit | 60 requests | 1 minuto |
| Mastodon | 300 requests | 5 minutos |

Configurables via variables de entorno.

## ğŸ“š Uso

### Flujo de Trabajo BÃ¡sico

#### 1. Crear un lineamiento
```bash
curl -X POST "http://localhost:8000/lineamientos/" \
  -H "Content-Type: application/json" \
  -H "X-API-Key: tu-api-key" \
  -d '{
    "nombre": "TecnologÃ­a IA 2025",
    "keywords": ["IA", "inteligencia artificial", "GPT", "machine learning"],
    "plataformas": ["youtube", "reddit", "mastodon"]
  }'
```

#### 2. Recolectar contenido (manual)
```bash
curl -X POST "http://localhost:8000/collect/lineamiento/{lineamiento_id}?hours_back=24" \
  -H "X-API-Key: tu-api-key"
```

O esperar la recolecciÃ³n automÃ¡tica (cada 30 minutos).

#### 3. Consultar tendencias
```bash
curl "http://localhost:8000/tendencias/?hours_back=24&solo_activas=true" \
  -H "X-API-Key: tu-api-key"
```

#### 4. Obtener tendencias agregadas
```bash
curl "http://localhost:8000/tendencias/agregadas?top_n=10" \
  -H "X-API-Key: tu-api-key"
```

#### 5. Ver estructura jerÃ¡rquica
```bash
curl "http://localhost:8000/tendencias/jerarquicas?hours_back=24" \
  -H "X-API-Key: tu-api-key"
```

## ğŸ“– DocumentaciÃ³n de API

### DocumentaciÃ³n Interactiva

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Endpoints Principales

| MÃ©todo | Endpoint | DescripciÃ³n |
|--------|----------|-------------|
| **Lineamientos** |||
| POST | `/lineamientos/` | Crear lineamiento |
| GET | `/lineamientos/` | Listar lineamientos |
| GET | `/lineamientos/{id}` | Obtener lineamiento |
| PUT | `/lineamientos/{id}` | Actualizar lineamiento |
| DELETE | `/lineamientos/{id}` | Eliminar lineamiento (soft delete) |
| POST | `/lineamientos/{id}/activate` | Reactivar lineamiento |
| **RecolecciÃ³n** |||
| POST | `/collect/lineamiento/{id}` | Recolectar todas las plataformas |
| POST | `/collect/lineamiento/{id}/platform/{platform}` | Recolectar plataforma especÃ­fica |
| POST | `/collect/all` | Recolectar todos los lineamientos |
| GET | `/collect/task/{task_id}` | Estado de tarea |
| **Tendencias** |||
| GET | `/tendencias/` | Listar tendencias con filtros |
| GET | `/tendencias/agregadas` | Tendencias agregadas por tema |
| GET | `/tendencias/jerarquicas` | Estructura jerÃ¡rquica completa |

Ver [API_DOCUMENTATION.md](API_DOCUMENTATION.md) para la documentaciÃ³n completa con ejemplos.

## ğŸ—ï¸ Arquitectura

### Diagrama de Componentes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FastAPI REST API                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Lineamientos â”‚  â”‚  RecolecciÃ³n â”‚  â”‚  Tendencias  â”‚          â”‚
â”‚  â”‚   (CRUD)     â”‚  â”‚   (Trigger)  â”‚  â”‚   (Query)    â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Celery Workers                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Collectors  â”‚  â”‚     NLP      â”‚  â”‚  Analytics   â”‚          â”‚
â”‚  â”‚  (YouTube,   â”‚  â”‚  (spaCy,     â”‚  â”‚ (Tendencias, â”‚          â”‚
â”‚  â”‚   Reddit,    â”‚  â”‚  sentiment,  â”‚  â”‚  ValidaciÃ³n) â”‚          â”‚
â”‚  â”‚  Mastodon)   â”‚  â”‚  BERTopic)   â”‚  â”‚              â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PostgreSQL 15 + TimescaleDB + Redis                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   Tables     â”‚  â”‚ Hypertables  â”‚  â”‚  Task Queue  â”‚          â”‚
â”‚  â”‚ Lineamientos â”‚  â”‚  Tendencias  â”‚  â”‚   (Redis)    â”‚          â”‚
â”‚  â”‚  Contenido   â”‚  â”‚  Aggregates  â”‚  â”‚              â”‚          â”‚
â”‚  â”‚    Temas     â”‚  â”‚              â”‚  â”‚              â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo de Datos

1. **RecolecciÃ³n** (cada 30 min)
   - Celery Beat dispara `collect_all_lineamientos`
   - Workers de `collectors` queue recolectan en paralelo
   - Contenido guardado en `contenido_recolectado`

2. **Procesamiento NLP** (cada hora)
   - Celery Beat dispara `process_pending_content`
   - Workers de `nlp` queue procesan con spaCy/sentiment
   - Temas guardados en `temas_identificados`
   - Demographics en `demografia`

3. **AnÃ¡lisis de Tendencias** (cada hora)
   - Celery Beat dispara `analyze_trends`
   - Workers de `analytics` queue calculan mÃ©tricas
   - Tendencias en tabla `tendencias` (hypertable)

4. **ValidaciÃ³n** (cada 6 horas)
   - Celery Beat dispara `validate_trends`
   - Workers consultan Google Trends
   - Validaciones en `validacion_tendencias`

## ğŸ› ï¸ Stack TecnolÃ³gico

### Backend
- **FastAPI** 0.104+ - Framework web asÃ­ncrono
- **Pydantic** 2.0+ - ValidaciÃ³n de datos
- **SQLAlchemy** 2.0+ - ORM
- **Alembic** 1.13+ - Migraciones de base de datos

### Procesamiento AsÃ­ncrono
- **Celery** 5.3+ - Task queue
- **Redis** 6.0+ - Message broker

### Base de Datos
- **PostgreSQL** 15+
- **TimescaleDB** - ExtensiÃ³n para series temporales

### NLP
- **spaCy** 3.7+ con modelo `es_core_news_md`
- **BERTopic** 0.16+ - Topic modeling
- **sentence-transformers** - Embeddings (RoBERTuito)
- **pysentimiento** 0.7+ - AnÃ¡lisis de sentimiento en espaÃ±ol

### APIs Externas
- **google-api-python-client** - YouTube Data API v3
- **praw** 7.7+ - Reddit API
- **Mastodon.py** 1.8+ - Mastodon API
- **pytrends** 4.9+ - Google Trends (unofficial)

### Herramientas
- **Poetry** - GestiÃ³n de dependencias
- **Docker** & **Docker Compose** - ContainerizaciÃ³n
- **pytest** - Testing
- **Flower** - Monitoring de Celery

## â° Tareas Programadas

| Tarea | Frecuencia | DescripciÃ³n |
|-------|------------|-------------|
| `collect_all_lineamientos` | Cada 30 min | Recolecta contenido de todas las plataformas |
| `process_pending_content` | Cada hora (en punto) | Procesa contenido con NLP |
| `analyze_trends` | Cada hora (min 15) | Analiza tendencias y calcula mÃ©tricas |
| `validate_trends` | Cada 6 horas (min 30) | Valida tendencias con Google Trends |
| `cleanup_old_data` | Diariamente (3:00 AM) | Limpia datos antiguos segÃºn retention policy |

## ğŸ§ª Testing

### Ejecutar tests
```bash
# Con Poetry
poetry run pytest

# Con coverage
poetry run pytest --cov=src --cov-report=html

# Tests especÃ­ficos
poetry run pytest tests/test_lineamientos.py -v
```

### Test Coverage Actual
- **Lineamientos**: 18 tests (CRUD completo)
- **Collectors**: Tests pendientes
- **NLP**: Tests pendientes
- **Analytics**: Tests pendientes

## ğŸš¢ Despliegue

### Docker Production

1. **Build imagen**
```bash
docker build -t trendsgpx-backend:latest .
```

2. **Configurar variables de entorno producciÃ³n**
```bash
cp .env.example .env.production
# Editar con valores de producciÃ³n
```

3. **Deploy con docker-compose**
```bash
docker-compose -f docker-compose.prod.yml up -d
```

### Consideraciones de ProducciÃ³n

- âœ… Cambiar `API_KEY` en `.env`
- âœ… Usar PostgreSQL gestionado (AWS RDS, Google Cloud SQL)
- âœ… Configurar Redis con persistencia
- âœ… Configurar logs centralizados (ELK, Datadog)
- âœ… Configurar monitoring (Prometheus + Grafana)
- âœ… Configurar backups automÃ¡ticos de PostgreSQL
- âœ… Usar HTTPS con certificado SSL
- âœ… Configurar rate limiting en API Gateway
- âœ… Escalar workers de Celery horizontalmente segÃºn carga

## ğŸ¤ Contribuir

Â¡Las contribuciones son bienvenidas! Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

### GuÃ­as de ContribuciÃ³n

- Seguir PEP 8 para cÃ³digo Python
- Agregar tests para nuevas funcionalidades
- Actualizar documentaciÃ³n segÃºn sea necesario
- Usar commits descriptivos

## ğŸ“ Licencia

Este proyecto estÃ¡ licenciado bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

## ğŸ‘¥ Autores

- **Juan Suarez** - *Desarrollo Inicial* - [@juansuarez-pragma](https://github.com/juansuarez-pragma)

## ğŸ™ Agradecimientos

- FastAPI por el excelente framework
- spaCy por las herramientas de NLP
- TimescaleDB por la optimizaciÃ³n de series temporales
- Comunidad de Python/FastAPI

## ğŸ“ Contacto

- GitHub Issues: [https://github.com/tu-usuario/trendsgpx-backend/issues](https://github.com/tu-usuario/trendsgpx-backend/issues)
- Email: tu-email@example.com

---

<div align="center">
Hecho con â¤ï¸ usando FastAPI, spaCy y TimescaleDB
</div>
